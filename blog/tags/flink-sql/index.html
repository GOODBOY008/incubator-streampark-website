<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-tags-post-list-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.3">
<title data-rh="true">7 posts tagged with &quot;FlinkSQL&quot; | Apache StreamPark (incubating)</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://streampark.apache.org/blog/tags/flink-sql"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" property="og:title" content="7 posts tagged with &quot;FlinkSQL&quot; | Apache StreamPark (incubating)"><meta data-rh="true" name="docusaurus_tag" content="blog_tags_posts"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_tags_posts"><link data-rh="true" rel="icon" href="/image/favicon.ico"><link data-rh="true" rel="canonical" href="https://streampark.apache.org/blog/tags/flink-sql"><link data-rh="true" rel="alternate" href="https://streampark.apache.org/blog/tags/flink-sql" hreflang="en"><link data-rh="true" rel="alternate" href="https://streampark.apache.org/zh-CN/blog/tags/flink-sql" hreflang="zh-CN"><link data-rh="true" rel="alternate" href="https://streampark.apache.org/blog/tags/flink-sql" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Apache StreamPark (incubating) RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Apache StreamPark (incubating) Atom Feed"><link rel="stylesheet" href="/assets/css/styles.75eac060.css">
<link rel="preload" href="/assets/js/runtime~main.f303b739.js" as="script">
<link rel="preload" href="/assets/js/main.41b6cbdd.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):window.matchMedia("(prefers-color-scheme: light)").matches?e("light"):e("dark")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/image/logo.png" alt="StreamPark Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/image/logo.png" alt="StreamPark Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div></a><a class="navbar__item navbar__link" href="/docs/intro">Documentation</a><a class="navbar__item navbar__link" href="/download">Download</a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Community</a><ul class="dropdown__menu"><li><a href="https://www.apache.org/foundation/policies/conduct" target="_blank" rel="noopener noreferrer" class="dropdown__link">Code of conduct</a></li><li><a class="dropdown__link" href="/community/contribution_guide/mailing_lists">Join the mailing lists</a></li><li><a class="dropdown__link" href="/community/contribution_guide/become_committer">Become A Committer</a></li><li><a class="dropdown__link" href="/community/contribution_guide/become_pmc_member">Become A PMC member</a></li><li><a class="dropdown__link" href="/community/contribution_guide/new_committer_process">New Committer Process</a></li><li><a class="dropdown__link" href="/community/contribution_guide/new_pmc_ember_process">New PMC Member Process</a></li><li><a class="dropdown__link" href="/community/submit_guide/document">Documentation Notice</a></li><li><a class="dropdown__link" href="/community/submit_guide/submit_code">Submit Code</a></li><li><a class="dropdown__link" href="/community/submit_guide/code_style_and_quality_guide">Code style and quality guide</a></li><li><a class="dropdown__link" href="/community/release/how_to_release">How to release</a></li><li><a class="dropdown__link" href="/community/release/how_to_verify_release">How to Verify Release</a></li></ul></div><a class="navbar__item navbar__link" href="/team">Team</a><a class="navbar__item navbar__link" href="/user">Users</a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">ASF</a><ul class="dropdown__menu"><li><a href="https://www.apache.org/" target="_blank" rel="noopener noreferrer" class="dropdown__link">Foundation</a></li><li><a href="https://www.apache.org/licenses/" target="_blank" rel="noopener noreferrer" class="dropdown__link">License</a></li><li><a href="https://www.apache.org/events/current-event" target="_blank" rel="noopener noreferrer" class="dropdown__link">Events</a></li><li><a href="https://www.apache.org/security/" target="_blank" rel="noopener noreferrer" class="dropdown__link">Security</a></li><li><a href="https://www.apache.org/foundation/sponsorship.html" target="_blank" rel="noopener noreferrer" class="dropdown__link">Sponsorship</a></li><li><a href="https://www.apache.org/foundation/policies/privacy.html" target="_blank" rel="noopener noreferrer" class="dropdown__link">Privacy</a></li><li><a href="https://www.apache.org/foundation/thanks.html" target="_blank" rel="noopener noreferrer" class="dropdown__link">Thanks</a></li></ul></div><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a><a href="https://github.com/apache/incubator-streampark/issues/507" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">FAQ</a><a href="https://github.com/apache/incubator-streampark" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>English</a><ul class="dropdown__menu"><li><a href="/blog/tags/flink-sql" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en">English</a></li><li><a href="/zh-CN/blog/tags/flink-sql" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-CN">简体中文</a></li></ul></div><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently dark mode)" aria-label="Switch between dark and light mode (currently dark mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/streampark-flink-on-k8s">StreamPark Flink on Kubernetes practice</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/flink-development-framework-streampark">Flink 开发利器 StreamPark</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/streampark-usercase-chinaunion">联通 Flink 实时计算平台化运维实践</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/streampark-usercase-bondex-with-paimon">海程邦达基于 Apache Paimon + StreamPark 的流式数仓实践</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/streampark-usercase-shunwang">StreamPark 在顺网科技的大规模生产实践</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/streampark-usercase-dustess">StreamPark 在尘锋信息的最佳实践，化繁为简极致体验</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/streampark-usercase-joyme">StreamPark 在 Joyme 的生产实践</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/streampark-usercase-haibo">StreamPark 一站式计算利器在海博科技的生产实践，助力智慧城市建设</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><header class="margin-bottom--xl"><h1>7 posts tagged with &quot;FlinkSQL&quot;</h1><a href="/blog/tags">View All Tags</a></header><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="Wuxin Technology was founded in January 2018. The current main business includes the research and development, design, manufacturing and sales of RELX brand products. With core technologies and capabilities covering the entire industry chain, RELX is committed to providing users with products that are both high quality and safe."><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/streampark-flink-on-k8s">StreamPark Flink on Kubernetes practice</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-10-24T15:06:09.000Z" itemprop="datePublished">October 24, 2023</time> · <!-- -->14 min read</div></header><div class="markdown" itemprop="articleBody"><p>Wuxin Technology was founded in January 2018. The current main business includes the research and development, design, manufacturing and sales of RELX brand products. With core technologies and capabilities covering the entire industry chain, RELX is committed to providing users with products that are both high quality and safe.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="why-choose-native-kubernetes"><strong>Why Choose Native Kubernetes</strong><a href="#why-choose-native-kubernetes" class="hash-link" aria-label="Direct link to why-choose-native-kubernetes" title="Direct link to why-choose-native-kubernetes">​</a></h2><p>Native Kubernetes offers the following advantages:</p><ul><li><p>Shorter Failover time</p></li><li><p>Resource hosting can be implemented without the need to manually create TaskManager Pods, which can be automatically destroyed</p></li><li><p>With more convenient HA, in Native Kubernetes mode after Flink version 1.12, you can rely on the Leader election mechanism of native Kubernetes to complete JobManager&#x27;s HA</p><p>The main difference between Native Kubernetes and Standalone Kubernetes lies in the way Flink interacts with Kubernetes and the resulting series of advantages. Standalone Kubernetes requires users to customize the Kubernetes resource description files of JobManager and TaskManager. When submitting a job, you need to use kubectl combined with the resource description file to start the Flink cluster. The Native Kubernetes mode Flink Client integrates a Kubernetes Client, which can directly communicate with the Kubernetes API Server to complete the creation of JobManager Deployment and ConfigMap. After JobManager Development is created, the Resource Manager module in it can directly communicate with the Kubernetes API Server to complete the creation and destruction of TaskManager pods and the elastic scaling of Taskmanager. Therefore, it is recommended to use Flink on Native Kubernetes mode to deploy Flink tasks in production environments.</p></li></ul><p><img loading="lazy" src="/assets/images/nativekubernetes_architecture-ad376f8ae79ab66d90d95742e8335d53.png" width="1080" height="401" class="img_ev3q"></p><p>When Flink On Kubernetes meets StreamPark</p><p>  Flink on Native Kubernetes currently supports Application mode and Session mode. Compared with the two, Application mode deployment avoids the resource isolation problem and client resource consumption problem of Session mode. Therefore, it is recommended to use Application Mode to deploy Flink tasks in <strong> production environments. </strong>Let’s take a look at the method of using the original script and the process of using StreamPark to develop and deploy a Flink on Native Kubernetes job.
Deploy Kubernetes using scripts</p><p>In the absence of a platform that supports Flink on Kubernetes task development and deployment, you need to use scripts to submit and stop tasks. This is also the default method provided by Flink. The specific steps are as follows:</p><ol><li>Prepare the kubectl and Docker command running environment on the Flink client node, create the Kubernetes Namespace and Service Account used to deploy the Flink job, and perform RBAC</li><li>Write a Dockerfile file to package the Flink base image and the user’s job Jar together</li></ol><div class="language-dockerfile codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-dockerfile codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">FROM flink:1.13.6-scala_2.11</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">RUN mkdir -p $FLINK_HOME/usrlib</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">COPY my-flink-job.jar $FLINK_HOME/usrlib/my-flink-job.jar</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><ol start="4"><li>Use Flink client script to start Flink tasks</li></ol><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">./bin/flink run-application </span><span class="token punctuation" style="color:rgb(248, 248, 242)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    --target kubernetes-application </span><span class="token punctuation" style="color:rgb(248, 248, 242)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    -Dkubernetes.namespace</span><span class="token operator">=</span><span class="token plain">flink-cluster </span><span class="token punctuation" style="color:rgb(248, 248, 242)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    -Dkubernetes.jobmanager.service-account</span><span class="token operator">=</span><span class="token plain">default </span><span class="token punctuation" style="color:rgb(248, 248, 242)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    -Dkubernetes.cluster-id</span><span class="token operator">=</span><span class="token plain">my-first-application-cluster </span><span class="token punctuation" style="color:rgb(248, 248, 242)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    -Dkubernetes.container.image</span><span class="token operator">=</span><span class="token plain">relx_docker_url/streamx/relx_flink_1.13.6-scala_2.11:latest </span><span class="token punctuation" style="color:rgb(248, 248, 242)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    -Dkubernetes.rest-service.exposed.type</span><span class="token operator">=</span><span class="token plain">NodePort </span><span class="token punctuation" style="color:rgb(248, 248, 242)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    local:///opt/flink/usrlib/my-flink-job.jar</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><ol start="5"><li>Use the Kubectl command to obtain the WebUI access address and JobId of the Flink job.</li></ol><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">kubectl -n flink-cluster get svc</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><ol start="6"><li>Stop the job using Flink command</li></ol><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">./bin/flink cancel</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    --target kubernetes-application</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    -Dkubernetes.cluster-id</span><span class="token operator">=</span><span class="token plain">my-first-application-cluster</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    -Dkubernetes.namespace</span><span class="token operator">=</span><span class="token plain">flink-cluster </span><span class="token operator">&lt;</span><span class="token plain">jobId</span><span class="token operator">&gt;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>  The above is the process of deploying a Flink task to Kubernetes using the most original script method provided by Flink. Only the most basic task submission is achieved. If it is to reach the production use level, there are still a series of problems that need to be solved, such as: the method is too Originally, it was unable to adapt to large batches of tasks, unable to record task checkpoints and real-time status tracking, difficult to operate and monitor tasks, had no alarm mechanism, and could not be managed in a centralized manner, etc.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-flink-on-kubernetes-using-streampark"><strong>Deploy Flink on Kubernetes using StreamPark</strong><a href="#deploy-flink-on-kubernetes-using-streampark" class="hash-link" aria-label="Direct link to deploy-flink-on-kubernetes-using-streampark" title="Direct link to deploy-flink-on-kubernetes-using-streampark">​</a></h2><p>  There will be higher requirements for using Flink on Kubernetes in enterprise-level production environments. Generally, you will choose to build your own platform or purchase related commercial products. No matter which solution meets the product capabilities: large-scale task development and deployment, status tracking, operation and maintenance monitoring , failure alarms, unified task management, high availability, etc. are common demands.</p><p>  In response to the above issues, we investigated open source projects in the open source field that support the development and deployment of Flink on Kubernetes tasks. During the investigation, we also encountered other excellent open source projects. After comprehensively comparing multiple open source projects, we came to the conclusion: <strong> Whether StreamPark is completed The overall performance such as speed, user experience, and stability are all very good, so we finally chose StreamPark as our one-stop real-time computing platform. </strong></p><p>  Let’s take a look at how StreamPark supports Flink on Kubernetes:</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="basic-environment-configuration"><strong>Basic environment configuration</strong><a href="#basic-environment-configuration" class="hash-link" aria-label="Direct link to basic-environment-configuration" title="Direct link to basic-environment-configuration">​</a></h3><p>  Basic environment configuration includes Kubernetes and Docker warehouse information as well as Flink client information configuration. The simplest way for the Kubernetes basic environment is to directly copy the .kube/config of the Kubernetes node to the StreamPark node user directory, and then use the kubectl command to create a Flink-specific Kubernetes Namespace and perform RBAC configuration.</p><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token comment" style="color:rgb(98, 114, 164)"># Create k8s namespace used by Flink jobs</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">kubectl create ns flink-cluster</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Bind RBAC resources to the default user</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">kubectl create clusterrolebinding flink-role-binding-default --clusterrole</span><span class="token operator">=</span><span class="token plain">edit --serviceaccount</span><span class="token operator">=</span><span class="token plain">flink-cluster:default</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Docker account information can be configured directly in the Docker Setting interface:</p><p><img loading="lazy" src="/assets/images/docker_setting-80acf43cf64fd390e4d50da8830671c0.png" width="1080" height="586" class="img_ev3q"></p><p>StreamPark can adapt to multi-version Flink job development. The Flink client can be configured directly on the StreamPark Setting interface:</p><p><img loading="lazy" src="/assets/images/flinkversion_setting-b170a43882590683ea0c7f109f909396.png" width="1080" height="352" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="job-development"><strong>Job development</strong><a href="#job-development" class="hash-link" aria-label="Direct link to job-development" title="Direct link to job-development">​</a></h3><p>After StreamPark has configured the basic environment, it only takes three steps to develop and deploy a Flink job:</p><p><img loading="lazy" src="/assets/images/development_process-476918cfd29159983fe26b36ef487895.png" width="1080" height="271" class="img_ev3q"></p><p>  StreamPark supports both Upload Jar and direct writing of Flink SQL jobs. <strong>Flink SQL jobs only need to enter SQL and dependencies. This method greatly improves the development experience and avoids problems such as dependency conflicts.</strong> This article does not focus on this part。</p><p>  Here you need to select the deployment mode as kubernetes application, and configure the following parameters on the job development page: The parameters in the red box are the basic parameters of Flink on Kubernetes.</p><p><img loading="lazy" src="/assets/images/kubernetes_base_parameters-1a28fec8d9d3dc57744324db4ef58551.png" width="1080" height="1104" class="img_ev3q"></p><p>  The following parameters are parameters related to Flink jobs and resources. The choice of Resolve Order is related to the code loading mode. For jobs uploaded by the Upload Jar developed by the DataStream API, choose to use Child-first, and for Flink SQL jobs, choose to use Parent-first loading.</p><p><img loading="lazy" src="/assets/images/flink_parameters-ff7790882a753bd88fbf5db9b775a0e3.png" width="1080" height="1133" class="img_ev3q"></p><p>  Finally, there are the following two heavyweight parameters. For Native Kubernetes, k8s-pod-template generally only requires pod-template configuration. Dynamic Option is a supplement to the pod-template parameters. For some personalized configurations, you can Configured in Dynamic Option. For more Dynamic Option, please directly refer to the Flink official website.</p><p><img loading="lazy" src="/assets/images/pod_template-722285f448ec8adc0fa939d0baea2d10.png" width="1080" height="1104" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="job-online"><strong>Job online</strong><a href="#job-online" class="hash-link" aria-label="Direct link to job-online" title="Direct link to job-online">​</a></h3><p>After the job development is completed, the job comes online. In this step, StreamPark has done a lot of work, as follows:</p><ul><li>Prepare environment</li><li>Dependency download in job</li><li>Build job (JAR package)</li><li>Build image</li><li>Push the image to the remote warehouse</li></ul><p><strong>For users: Just click the cloud-shaped online button in the task list</strong></p><p><img loading="lazy" src="/assets/images/operation-067a84b9b5b1491206780076f98e6f8d.png" width="1080" height="573" class="img_ev3q"></p><p>We can see a series of work done by StreamPark when building and pushing the image.: <strong>Read the configuration, build the image, and push the image to the remote warehouse...</strong> I want to give StreamPark a big thumbs up!</p><p><img loading="lazy" src="/assets/images/step_details-301b14f2dbfa9c41f4c0e75a9086f0a4.png" width="948" height="1866" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="assignment-submission"><strong>Assignment submission</strong><a href="#assignment-submission" class="hash-link" aria-label="Direct link to assignment-submission" title="Direct link to assignment-submission">​</a></h3><p>  Finally, you only need to click the start Application button in Operation to start a Flink on Kubernetes job. After the job is successfully started, click on the job name to jump to the Jobmanager WebUI page. The whole process is very simple and smooth.</p><p><img loading="lazy" src="/assets/images/homework_submit-1f05dacf0fedfd1423f89ca2dec28437.png" width="1080" height="698" class="img_ev3q"></p><p>  The entire process only requires the above three steps to complete the development and deployment of a Flink on Kubernetes job on StreamPark. StreamPark&#x27;s support for Flink on Kubernetes goes far beyond simply submitting a task.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="job-management"><strong>Job management</strong><a href="#job-management" class="hash-link" aria-label="Direct link to job-management" title="Direct link to job-management">​</a></h3><p><strong>After the job is submitted, StreamPark can obtain the latest checkpoint address of the task, the running status of the task, and the real-time resource consumption information of the cluster in real time. It can very conveniently start and stop the running task with one click, and supports recording the savepoint location when stopping the job. , as well as functions such as restoring the state from savepoint when restarting, thus ensuring the data consistency of the production environment and truly possessing the one-stop development, deployment, operation and maintenance monitoring capabilities of Flink on Kubernetes.</strong></p><p>Next, let’s take a look at how StreamPark supports this capability:</p><ul><li><p><strong>Record checkpoint in real time</strong></p><p>After the job is submitted, sometimes it is necessary to change the job logic but to ensure data consistency, then the platform needs to have the ability to record the location of each checkpoint in real time, as well as the ability to record the last savepoint location when stopped. StreamPark is on Flink on Kubernetes This function is implemented very well. By default, checkpoint information will be obtained and recorded in the corresponding table every 5 seconds, and according to the policy of retaining the number of checkpoints in Flink, only state.checkpoints.num-retained will be retained, and the excess will be deleted. There is an option to check the savepoint when the task is stopped. If the savepoint option is checked, the savepoint operation will be performed when the task is stopped, and the specific location of the savepoint will also be recorded in the table.</p><p>The root path of the default savepoint only needs to be configured in the Flink Home flink-conf.yaml file to automatically identify it. In addition to the default address, the root path of the savepoint can also be customized and specified when stopping.</p></li></ul><p><img loading="lazy" src="/assets/images/savepoint-b0288f5293875d156f20dbe768384076.png" width="1080" height="446" class="img_ev3q"></p><p><img loading="lazy" src="/assets/images/checkpoint-acf7379b24a3bac6695a517b425f466b.png" width="1080" height="479" class="img_ev3q"></p><ul><li><p><strong>Track running status in real time</strong></p><p>For challenges in the production environment, a very important point is whether monitoring is in place, especially for Flink on Kubernetes. This is very important and is the most basic capability. StreamPark can monitor the running status of Flink on Kubernetes jobs in real time and display it to users on the platform. Tasks can be easily retrieved based on various running statuses on the page.</p></li></ul><p><img loading="lazy" src="/assets/images/run_status-5a663d9169c0bce8f4cfb993db77ae59.png" width="1080" height="617" class="img_ev3q"></p><ul><li><p><strong>Complete alarm mechanism</strong></p><p>In addition, StreamPark also has complete alarm functions: supporting email, DingTalk, WeChat and SMS, etc. This is also an important reason why the company chose StreamPark as the one-stop platform for Flink on Kubernetes after initial research.</p></li></ul><p><img loading="lazy" src="/assets/images/alarm-c2104c1839a1b4bb668d48f092f25faa.png" width="1080" height="393" class="img_ev3q"></p><p>  From the above, we can see that StreamPark has the capabilities to support the development and deployment process of Flink on Kubernetes, including: <strong> job development capabilities, deployment capabilities, monitoring capabilities, operation and maintenance capabilities, exception handling capabilities, etc. StreamPark provides a relatively complete set of s solution. And it already has some CICD/DevOps capabilities, and the overall completion level continues to improve. It is a product that supports the full link of Flink on Kubernetes one-stop development, deployment, operation and maintenance work in the entire open source field. StreamPark is worthy of praise. </strong></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="streamparks-implementation-in-wuxin-technology"><strong>StreamPark’s implementation in Wuxin Technology</strong><a href="#streamparks-implementation-in-wuxin-technology" class="hash-link" aria-label="Direct link to streamparks-implementation-in-wuxin-technology" title="Direct link to streamparks-implementation-in-wuxin-technology">​</a></h2><p>  StreamPark was launched late in Wuxin Technology. It is currently mainly used for the development and deployment of real-time data integration jobs and real-time indicator calculation jobs. There are Jar tasks and Flink SQL tasks, all deployed using Native Kubernetes; data sources include CDC, Kafka, etc., and Sink end There are Maxcompute, kafka, Hive, etc. The following is a screenshot of the company&#x27;s development environment StreamPark platform:</p><p><img loading="lazy" src="/assets/images/screenshot-2906914515b810aadb10db951d4f02bd.png" width="1080" height="706" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="problems-encountered">Problems encountered<a href="#problems-encountered" class="hash-link" aria-label="Direct link to Problems encountered" title="Direct link to Problems encountered">​</a></h2><p>  Any new technology has a process of exploration and pitfalls. The experience of failure is precious. Here are some pitfalls and experiences that StreamPark has stepped into during the implementation of fog core technology. **The content of this section is not only about StreamPark. I believe it will bring some reference to all friends who use Flink on Kubernetes.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="faqs-are-summarized-below"><strong>FAQs are summarized below</strong><a href="#faqs-are-summarized-below" class="hash-link" aria-label="Direct link to faqs-are-summarized-below" title="Direct link to faqs-are-summarized-below">​</a></h3><ul><li><strong>Kubernetes pod failed to pull the image</strong></li></ul><p>The main problem is that Kubernetes pod-template lacks docker’s imagePullSecrets</p><ul><li><p><strong>Scala version inconsistent</strong></p><p>Since StreamPark deployment requires a Scala environment, and Flink SQL operation requires the Flink SQL Client provided by StreamPark, it is necessary to ensure that the Scala version of the Flink job is consistent with the Scala version of StreamPark.</p></li><li><p><strong>Be aware of class conflicts</strong></p><p>When developing Flink SQL jobs, you need to pay attention to whether there are any class conflicts between the Flink image and the Flink connector and UDF. The best way to avoid class conflicts is to make the Flink image and the commonly used Flink connector and user UDF into a usable basic image. After that, other Flink SQL jobs can be reused directly.</p></li><li><p><strong>How to store checkpoint without Hadoop environment?</strong></p><p>HDFS, Alibaba Cloud OSS/AWS S3 can both perform checkpoint and savepoint storage. The Flink basic image already has support for OSS and S3. If you do not have HDFS, you can use Alibaba Cloud OSS or S3 to store status and checkpoint and savepoint data. You only need to use Flink Simply configure it in the dynamic parameters.</p></li></ul><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">-Dstate.backend</span><span class="token operator">=</span><span class="token plain">rocksdb</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">-Dcontainerized.master.env.ENABLE_BUILT_IN_PLUGINS</span><span class="token operator">=</span><span class="token plain">flink-oss-fs-hadoop-1.13.6.jar</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">-Dcontainerized.taskmanager.env.ENABLE_BUILT_IN_PLUGINS</span><span class="token operator">=</span><span class="token plain">flink-oss-fs-hadoop-1.13.6.jar</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">-Dfs.oss.endpoint</span><span class="token operator">=</span><span class="token plain">xxyy.aliyuncs.com</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">-Dfs.oss.accessKeyId</span><span class="token operator">=</span><span class="token plain">xxxxxxxxxx</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">-Dfs.oss.accessKeySecret</span><span class="token operator">=</span><span class="token plain">xxxxxxxxxx</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">-Dstate.checkpoints.dir</span><span class="token operator">=</span><span class="token plain">oss://realtime-xxx/streamx/dev/checkpoints/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">-Dstate.savepoints.dir</span><span class="token operator">=</span><span class="token plain">oss://realtime-xxx/streamx/dev/savepoints/</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><ul><li><strong>The changed code did not take effect after it was republished</strong></li></ul><p>This issue is related to the Kubernetes pod image pull policy. It is recommended to set the Pod image pull policy to Always:</p><p>‍-Dkubernetes.container.image.pull-policy=Always</p><ul><li><p><strong>Each restart of the task will result in one more Job instance</strong></p><p>Under the premise that kubernetes-based HA is configured, when you need to stop the Flink task, you need to use cancel of StreamPark. Do not delete the Deployment of the Flink task directly through the kubernetes cluster. Because Flink&#x27;s shutdown has its own shutdown process, when deleting a pod, the corresponding configuration files in the Configmap will also be deleted. Direct deletion of the pod will result in the remnants of the Configmap. When a task with the same name is restarted, two identical jobs will appear because at startup, the task will load the remaining configuration files and try to restore the closed task.</p></li><li><p><strong>How to implement kubernetes pod domain name access</strong></p></li></ul><p>Domain name configuration only needs to be configured in pod-template according to Kubernetes resources. I can share with you a pod-template.yaml template that I summarized based on the above issues:</p><div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token key atrule">apiVersion</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> v1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token key atrule">kind</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> Pod</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token key atrule">metadata</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">name</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> pod</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">template</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token key atrule">spec</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">serviceAccount</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> default</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">containers</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> flink</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">main</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">container</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">image</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">imagePullSecrets</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> regsecret</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">hostAliases</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain"> </span><span class="token key atrule">ip</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;192.168.0.1&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token key atrule">hostnames</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;node1&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain"> </span><span class="token key atrule">ip</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;192.168.0.2&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token key atrule">hostnames</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;node2&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain"> </span><span class="token key atrule">ip</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;192.168.0.3&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token key atrule">hostnames</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;node3&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="best-practices"><strong>Best Practices</strong><a href="#best-practices" class="hash-link" aria-label="Direct link to best-practices" title="Direct link to best-practices">​</a></h3><p>  Many of RELX&#x27;s big data components are based on Alibaba Cloud, such as Maxcompute and Alibaba Cloud Redis. At the same time, our Flink SQL jobs need to use some UDFs. At first, we adopted the method of using Flink Base image + maven dependency + upload udf jar, but in practice we encountered some problems such as version conflicts and class conflicts. At the same time, if it is a large-volume job, the development efficiency of this method is relatively low. Finally, we packaged the commonly used Flink connectors, udf and Flink base image at the company level into a company-level base image. New Flink SQL jobs can directly write Flink SQL after using this base image, which greatly improves development efficiency.</p><p><strong>Let’s share a simple step to create a basic image：</strong></p><p><strong>1. Prepare the required JAR</strong></p><p>Place the commonly used Flink Connector Jar and the user Udf Jar in the same folder lib. The following are some commonly used connector packages in Flink 1.13.6</p><div class="language-jar codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-jar codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">bigdata-udxf-1.0.0-shaded.jar</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">flink-connector-jdbc_2.11-1.13.6.jar</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">flink-sql-connector-kafka_2.11-1.13.6.jar</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">flink-sql-connector-mysql-cdc-2.0.2.jar</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">hudi-flink-bundle_2.11-0.10.0.jar</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">ververica-connector-odps-1.13-vvr-4.0.7.jar</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">ververica-connector-redis-1.13-vvr-4.0.7.jar</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p><strong>2. Prepare Dockerfile</strong></p><p>Create a Dockerfile file and place the Dockerfile file in the same folder as the above folder</p><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">FROM flink:1.13.6-scala_2.11COPY lib </span><span class="token variable" style="color:rgb(189, 147, 249);font-style:italic">$FLINK_HOME</span><span class="token plain">/lib/</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p><strong>3. Create a basic image and push it to a private warehouse</strong></p><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token function" style="color:rgb(80, 250, 123)">docker</span><span class="token plain"> login --username</span><span class="token operator">=</span><span class="token plain">xxxdocker </span><span class="token punctuation" style="color:rgb(248, 248, 242)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">build -t udf_flink_1.13.6-scala_2.11:latest </span><span class="token punctuation" style="color:rgb(248, 248, 242)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">.docker tag udf_flink_1.13.6-scala_2.11:latest </span><span class="token punctuation" style="color:rgb(248, 248, 242)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">k8s-harbor.xxx.com/streamx/udf_flink_1.13.6-scala_2.11:latestdocker </span><span class="token punctuation" style="color:rgb(248, 248, 242)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">push k8s-harbor.xxx.com/streamx/udf_flink_1.13.6-scala_2.11:latest</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="future-expectations"><strong>Future Expectations</strong><a href="#future-expectations" class="hash-link" aria-label="Direct link to future-expectations" title="Direct link to future-expectations">​</a></h2><ul><li><strong>StreamPark supports Flink job metric monitoring</strong></li></ul><p>It would be great if StreamPark could connect to Flink Metric data and display Flink’s real-time consumption data at every moment on the StreamPark platform.</p><ul><li><p><strong>StreamPark supports Flink job log persistence</strong></p><p>For Flink deployed to YARN, if the Flink program hangs, we can go to YARN to view the historical logs. However, for Kubernetes, if the program hangs, the Kubernetes pod will disappear and there will be no way to check the logs. Therefore, users need to use tools on Kubernetes for log persistence. It would be better if StreamPark supports the Kubernetes log persistence interface.</p></li><li><p><strong>Improvement of the problem of too large image</strong></p></li></ul><p>StreamPark&#x27;s current image support for Flink on Kubernetes jobs is to combine the basic image and user code into a Fat image and push it to the Docker warehouse. The problem with this method is that it takes a long time when the image is too large. It is hoped that the basic image can be restored in the future. There is no need to hit the business code together every time, which can greatly improve development efficiency and save costs.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/stream-park">StreamPark</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/生产实践">生产实践</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/flink-sql">FlinkSQL</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/kubernetes">Kubernetes</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="Hadoop 体系虽然在目前应用非常广泛，但架构繁琐、运维复杂度过高、版本升级困难，且由于部门原因，数据中台需求排期较长，我们急需探索敏捷性开发的数据平台模式。在目前云原生架构的普及和湖仓一体化的大背景下，我们已经确定了将 Doris 作为离线数据仓库，将 TiDB（目前已经应用于生产）作为实时数据平台，同时因为 Doris 具有 on MySQL 的 ODBC 能力，所以又可以对外部数据库资源进行整合，统一对外输出报表"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/flink-development-framework-streampark">Flink 开发利器 StreamPark</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-10-24T15:06:09.000Z" itemprop="datePublished">October 24, 2023</time> · <!-- -->15 min read</div></header><div class="markdown" itemprop="articleBody"><br><h1>1. 背景</h1><p>Hadoop 体系虽然在目前应用非常广泛，但架构繁琐、运维复杂度过高、版本升级困难，且由于部门原因，数据中台需求排期较长，我们急需探索敏捷性开发的数据平台模式。在目前云原生架构的普及和湖仓一体化的大背景下，我们已经确定了将 Doris 作为离线数据仓库，将 TiDB（目前已经应用于生产）作为实时数据平台，同时因为 Doris 具有 on MySQL 的 ODBC 能力，所以又可以对外部数据库资源进行整合，统一对外输出报表</p><p><img loading="lazy" src="/assets/images/doris-4baaea78343b928b0a798ae9238c489f.png" width="1200" height="738" class="img_ev3q"></p><center style="color:gray">(这里借用一下 Doris 官方的架构图)</center><br><br><h1>2. 遇到的问题</h1><p>在数据引擎上，我们确定使用 Spark 和 Flink</p><ul><li>使用 Spark on K8s client 客户端模式做离线数据处理</li><li>使用 Flink on K8s Native-Application/Session 模式做实时任务流管理</li></ul><p>在这里，实际上有一些问题我们一直没有彻底解决：</p><p>用过 Native-Application 模式的朋友都知道，每提交一个任务，都需要打包新的镜像，提交到私有仓库，然后再调用 Flink Run 指令沟通 K8s，去拉取镜像运行 Pod。任务提交之后，还需要去 K8s 查看 log, 但是：</p><ol><li>任务运行监控怎么处理？</li><li>使用 Cluster 模式还是 NodePort 暴露端口访问 Web UI？</li><li>提交任务能否简化打包镜像的流程?</li><li>如何减少开发压力？</li></ol><br><br><h1>3. 解决问题的过程</h1><p>以上的这些其实都是需要解决的问题，如果单纯地使用命令行去提交每个任务，是不现实的，任务量大了，会变得不可维护。如何解决这些问题变成一个不得不面对的问题。</p><br><h2 class="anchor anchorWithStickyNavbar_LWe7" id="简化镜像构建">简化镜像构建<a href="#简化镜像构建" class="hash-link" aria-label="Direct link to 简化镜像构建" title="Direct link to 简化镜像构建">​</a></h2><p>首先，针对 Flink 原生镜像需要二次 build 的问题：我们利用了 MinIO 作为外部存储，并使用 s3-fuse 通过 DaemonSet 的方式直接挂载在了每个宿主节点上，我们所需要提交的 jar 包都可以放到上面统一管理。这样的话，即使扩缩容 Flink 节点，也能实现 S3 挂载自动伸缩。</p><p><img loading="lazy" src="/assets/images/k8s-9a28cd8f0e9c996501193f591ebe22b0.png" width="1141" height="582" class="img_ev3q"></p><p>Flink 从 1.13 版本开始，就支持 Pod Template，我们可以在 Pod Template 中利用数据卷挂载的方式再将宿主机目录挂载到每个 pod 中，从而无需镜像打包而直接在 K8s 上运行 Flink 程序。如上图，我们将 S3 先通过 s3-fuse Pod 挂载在 Node 1、Node 2 的 <code>/mnt/data-s3fs</code> 目录下，然后再将 <code>/mnt/data-s3fs</code> 挂载到 Pod A 中。</p><p>但是，因为对象存储随机写入或追加文件需要重写整个对象，导致这种方式仅适合于频繁读。而这刚好满足我们现在的场景。</p><br><h2 class="anchor anchorWithStickyNavbar_LWe7" id="引入-streampark">引入 StreamPark<a href="#引入-streampark" class="hash-link" aria-label="Direct link to 引入 StreamPark" title="Direct link to 引入 StreamPark">​</a></h2><p>之前我们写 Flink SQL 基本上都是使用 Java 包装 SQL，打 jar 包，提交到 S3 平台上。通过命令行方式提交代码，但这种方式始终不友好，流程繁琐，开发和运维成本太大。我们希望能够进一步简化流程，将 Flink TableEnvironment 抽象出来，有平台负责初始化、打包运行 Flink 任务，实现 Flink 应用程序的构建、测试和部署自动化。</p><p>这是个开源兴起的时代，我们自然而然的将目光投向开源领域中：在一众开源项目中，经过对比各个项目综合评估发现 <span style="color:red"> Zeppelin </span> 和 <span style="color:red"> StreamPark </span> 这两个项目对 Flink 的支持较为完善，都宣称支持 <span style="color:red"> Flink on K8s </span>，最终进入到我们的目标选择范围中，以下是两者在 K8s 相关支持的简单比较（目前如果有更新，麻烦批评指正）。</p><table><thead><tr><td>功能</td><td>Zeppelin</td><td>StreamPark</td></tr></thead><tbody><tr><td>任务状态监控</td><td><span style="color:red"> 稍低 </span>，不能作为任务状态监控工具</td><td><span style="color:red"> 较高 </span></td></tr><tr><td>任务资源管理</td><td><span style="color:red"> 无 </span></td><td><span style="color:red"> 有 </span>，但目前版本还不是很健全</td></tr><tr><td>本地化部署</td><td><span style="color:red"> 稍低 </span>，on K8s 模式只能将 Zeppelin 部署在 K8s 中，否则就需要打通 Pod 和外部网络，但是这在生产环境中很少这样做的</td><td><span style="color:red"> 可以本地化部署 </span></td></tr><tr><td>多语言支持</td><td><span style="color:red"> 较高 </span>，支持 Python/Scala/Java 多语言</td><td><span style="color:red"> 一般 </span>，目前 K8s 模式和 YARN 模式同时支持 FlinkSQL，并可以根据自身需求，使用 Java/Scala 开发 DataStream</td></tr><tr><td>Flink WebUI 代理</td><td><span style="color:red"> 目前还支持的不是很完整 </span>，主开发大佬目前是考虑整合 Ingress</td><td><span style="color:red"> 较好 </span>，目前支持 ClusterIp/NodePort/LoadBalance 模式</td></tr><tr><td>学习成本</td><td><span style="color:red"> 成本较低 </span>，需要增加额外的参数学习，这个和原生的 FlinkSQL 在参数上有点区别</td><td><span style="color:red"> 无成本 </span>，K8s 模式下 FlinkSQL 为原生支持的 SQL 格式；同时支持 Custome-Code（用户编写代码开发Datastream/FlinkSQL 任务）</td></tr><tr><td>Flink 多版本支持</td><td><span style="color:red"> 支持 </span></td><td><span style="color:red"> 支持 </span></td></tr><tr><td>Flink 原生镜像侵入</td><td><span style="color:red"> 有侵入 </span>，需要在 Flink 镜像中提前部署 jar 包，会同 JobManager 启动在同一个 Pod 中，和 zeppelin-server 通信</td><td><span style="color:red"> 无侵入 </span>，但是会产生较多镜像，需要定时清理</td></tr><tr><td>代码多版本管理</td><td><span style="color:red"> 支持 </span></td><td><span style="color:red"> 支持 </span></td></tr></tbody></table><center style="color:gray">（PS: 此处仅从调研用户角度出发，我们对双方开发都保持极大的尊重）</center><br><p>调研过程中，我们与两者的主开发人员都进行了多次沟通。经过我们反复研究之后，还是决定将 StreamPark 作为我们目前的 Flink 开发工具来使用。</p><video src="http://assets.streamxhub.com/streamx-video.mp4" controls="" width="100%" height="100%"></video><center style="color:gray">(StreamPark 官网的闪屏)</center><br><p>经过开发同学长时间开发测试，StreamPark 目前已经具备：</p><ul><li>完善的<span style="color:red">SQL 校验功能</span></li><li>实现了<span style="color:red">自动 build/push 镜像</span></li><li>使用自定义类加载器，通过 Child-first 加载方式 <span style="color:red">解决了 YARN 和 K8s 两种运行模式</span>、<span style="color:red">支持了自由切换 Flink 多版本</span></li><li>与 Flink-Kubernetes 进行深度整合，提交任务后返回 WebUI，通过 remote rest api + remote K8s，<span style="color:red">追踪任务执行状态</span></li><li>同时支持了 <span style="color:red">Flink 1.12、1.13、1.14 等版本</span></li></ul><p>以上基本解决了我们目前开发和运维中存在的大部分问题。</p><video src="http://assets.streamxhub.com/streamx-1.2.0.mp4" controls="" width="100%" height="100%"></video><center style="color:gray">(StreamPark 对 Flink 多版本的支持演示视频)</center><br><p>在目前最新发布的 1.2.0 版本中，StreamPark 较为完善地支持了 K8s-Native-Application 和 K8s-Session-Application 模式。</p><video src="http://assets.streamxhub.com/streamx-k8s.mp4" controls="" width="100%" height="100%"></video><center style="color:gray">(StreamPark K8s 部署演示视频)</center><br><h3 class="anchor anchorWithStickyNavbar_LWe7" id="k8s-native-application-模式">K8s Native Application 模式<a href="#k8s-native-application-模式" class="hash-link" aria-label="Direct link to K8s Native Application 模式" title="Direct link to K8s Native Application 模式">​</a></h3><p>在 StreamPark 中，我们只需要配置相应的参数，并在 Maven POM 中填写相应的依赖，或者上传依赖 jar 包，点击 Apply，相应的依赖就会生成。这就意味着我们也可以将所有使用的 UDF 打成 jar 包，以及各种 connector.jar，直接在 SQL 中使用。如下图:</p><p><img loading="lazy" src="/assets/images/dependency-a3b9ff29795acb8a1fd4ed6bb773d53e.png" width="1080" height="461" class="img_ev3q"></p><p>SQL 校验能力和 Zeppelin 基本一致:</p><p><img loading="lazy" src="/assets/images/sqlverify-7e12cf343c9c81fcbc2e20f8d7588f1b.png" width="1080" height="619" class="img_ev3q"></p><p>我们也可以指定资源，指定 Flink Run 中的动态参数 Dynamic Option，甚至参数可以整合 Pod  Template</p><p><img loading="lazy" src="/assets/images/pod-d46370aaff2c34c4fe6a584c0524b28e.png" width="1080" height="574" class="img_ev3q"></p><p>程序保存后，点击运行时，也可以指定 savepoint。任务提交成功后，StreamPark 会根据 FlinkPod 网络 Exposed Type（loadBalancer/NodePort/ClusterIp），返回相应的 WebURL，从而自然的实现 WebUI 跳转。但是，目前因为线上私有 K8s 集群出于安全性考虑，尚未打通 Pod 与客户端节点网络（目前也没有这个规划）。所以么，我们只使用 NodePort。如果后续任务数过多，有使用 ClusterIP 的需求的话，我们可能会将 StreamPark 部署在 K8s，或者同 Ingress 做进一步整合。</p><p><img loading="lazy" src="/assets/images/start-71fbb288851d022c450a6bd34e8b4dc2.png" width="1080" height="522" class="img_ev3q"></p><p>注意：K8s master 如果使用 vip 做均衡代理的情况下，Flink 1.13 版本会返回 vip 的 ip 地址，在 1.14 版本中已经修复该问题。</p><p>下面是 K8s Application 模式下具体提交流程</p><p><img loading="lazy" src="/assets/images/flow-c2227ba0cc1f59f78e2164fdb3657223.png" width="1080" height="949" class="img_ev3q"></p><center style="color:gray">（以上是依据个人理解绘制的任务提交流程图，如有错误，敬请谅解）</center><br><h3 class="anchor anchorWithStickyNavbar_LWe7" id="k8s-native-session-模式">K8s Native Session 模式<a href="#k8s-native-session-模式" class="hash-link" aria-label="Direct link to K8s Native Session 模式" title="Direct link to K8s Native Session 模式">​</a></h3><p>StreamPark 还较好地支持了 <span style="color:red"> K8s Native-Sesson 模式</span>，这为我们后续做离线 FlinkSQL 开发或部分资源隔离做了较好的技术支持。</p><p>Native-Session 模式需要事先使用 Flink 命令创建一个运行在 K8s 中的 Flink 集群。如下：</p><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">./kubernetes-session.sh </span><span class="token punctuation" style="color:rgb(248, 248, 242)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">-Dkubernetes.cluster-id</span><span class="token operator">=</span><span class="token plain">flink-on-k8s-flinkSql-test </span><span class="token punctuation" style="color:rgb(248, 248, 242)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">-Dkubernetes.context</span><span class="token operator">=</span><span class="token plain">XXX </span><span class="token punctuation" style="color:rgb(248, 248, 242)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">-Dkubernetes.namespace</span><span class="token operator">=</span><span class="token plain">XXXX </span><span class="token punctuation" style="color:rgb(248, 248, 242)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">-Dkubernetes.service-account</span><span class="token operator">=</span><span class="token plain">XXXX </span><span class="token punctuation" style="color:rgb(248, 248, 242)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">-Dkubernetes.container.image</span><span class="token operator">=</span><span class="token plain">XXXX </span><span class="token punctuation" style="color:rgb(248, 248, 242)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">-Dkubernetes.container.image.pull-policy</span><span class="token operator">=</span><span class="token plain">Always </span><span class="token punctuation" style="color:rgb(248, 248, 242)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">-Dkubernetes.taskmanager.node-selector</span><span class="token operator">=</span><span class="token plain">XXXX </span><span class="token punctuation" style="color:rgb(248, 248, 242)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">-Dkubernetes.rest-service.exposed.type</span><span class="token operator">=</span><span class="token plain">Nodeport</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p><img loading="lazy" src="/assets/images/flinksql-13b242feb3803b15e6698635a79065b4.png" width="1080" height="664" class="img_ev3q"></p><p>如上图，使用该 ClusterId 作为 StreamPark 的任务参数 Kubernetes ClusterId。保存提交任务后，任务会很快处于 Running 状态：</p><p><img loading="lazy" src="/assets/images/detail-e9ee4c14e45068bea5e1edabec596bee.png" width="1080" height="529" class="img_ev3q"></p><p>我们顺着 application info 的 WebUI 点击跳转：</p><p><img loading="lazy" src="/assets/images/dashboard-78745d8d3ebe422b166a17631bfbe622.png" width="1080" height="520" class="img_ev3q"></p><p>可以看到，其实 StreamPark 是将 jar 包通过 REST API 上传到 Flink 集群上，并调度执行任务的。</p><br><h3 class="anchor anchorWithStickyNavbar_LWe7" id="custom-code-模式">Custom Code 模式<a href="#custom-code-模式" class="hash-link" aria-label="Direct link to Custom Code 模式" title="Direct link to Custom Code 模式">​</a></h3><p>另我们惊喜的是，StreamPark 还支持代码编写 DataStream/FlinkSQL 任务。对于特殊需求，我们可以自己写 Java/Scala 实现。可以根据 StreamPark 推荐的脚手架方式编写任务，也可以编写一个标准普通的 Flink 任务，通过这种方式我们可以将代码管理交由 git 实现，平台可以用来自动化编译打包与部署。当然，如果能用 SQL 实现的功能，我们会尽量避免自定义 DataStream，减少不必要的运维麻烦。</p><br><br><h1>4. 意见和规划</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="改进意见">改进意见<a href="#改进意见" class="hash-link" aria-label="Direct link to 改进意见" title="Direct link to 改进意见">​</a></h2><p>当然 StreamPark 还有很多需要改进的地方，就目前测试来看：</p><ul><li><strong>资源管理还有待加强</strong>：多文件系统jar包等资源管理功能尚未添加，任务版本功能有待加强。</li><li><strong>前端 button 功能还不够丰富</strong>：比如任务添加后续可以增加复制等功能按钮。</li><li><strong>任务提交日志也需要可视化展示</strong>：任务提交伴随着加载 class 文件，打 jar 包，build 镜像，提交镜像，提交任务等过程，每一个环节出错，都会导致任务的失败，但是失败日志往往不明确，或者因为某种原因导致异常未正常抛出，没有转换任务状态，用户会无从下手改进。</li></ul><p>众所周知，一个新事物的出现一开始总会不是那么完美。尽管有些许问题和需要改进的 point，但是瑕不掩瑜，我们仍然选择 StreamPark 作为我们的 Flink DevOps，我们也将会和主开发人员一道共同完善 StreamPark，也欢迎更多的人来使用，为 StreamPark 带来更多进步。</p><br><h2 class="anchor anchorWithStickyNavbar_LWe7" id="未来规划">未来规划<a href="#未来规划" class="hash-link" aria-label="Direct link to 未来规划" title="Direct link to 未来规划">​</a></h2><ul><li>我们会继续跟进 Doris，并将业务数据 + 日志数据统一入 Doris，通过 Flink 实现湖仓一体；</li><li>我们也会逐步将探索 StreamPark 同 DolphinScheduler 2.x 进行整合，完善DolphinScheduler 离线任务，逐步用 Flink 替换掉 Spark，实现真正的流批一体；</li><li>基于我们自身在 S3 上的探索积累，fat-jar 包 build 完成之后不再构建镜像，直接利用 Pod Tempelet 挂载 PVC 到 Flink Pod 中的目录，进一步优化代码提交流程；</li><li>将 StreamPark 持续应用到我们生产中，并汇同社区开发人员，共同努力，增强 StreamPark 在 Flink 流上的开发部署能力与运行监控能力，努力把 StreamPark 打造成一个功能完善的流数据  DevOps。</li></ul><p>附：</p><p>StreamPark GitHub：<a href="https://github.com/apache/incubator-streampark" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-streampark</a> <br>
Doris GitHub：<a href="https://github.com/apache/doris" target="_blank" rel="noopener noreferrer">https://github.com/apache/doris</a></p><p><img loading="lazy" src="/assets/images/author-c3dabbb31d7cea1b5164a75a94ca3008.png" width="900" height="500" class="img_ev3q"></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/stream-park">StreamPark</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/data-stream">DataStream</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/flink-sql">FlinkSQL</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="摘要：本文整理自联通数科实时计算团队负责人、Apache StreamPark Committer 穆纯进在 Flink Forward Asia 2022 平台建设专场的分享，本篇内容主要分为四个部分："><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/streampark-usercase-chinaunion">联通 Flink 实时计算平台化运维实践</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-10-24T15:06:09.000Z" itemprop="datePublished">October 24, 2023</time> · <!-- -->23 min read</div></header><div class="markdown" itemprop="articleBody"><p><strong>摘要：</strong>本文整理自联通数科实时计算团队负责人、Apache StreamPark Committer 穆纯进在 Flink Forward Asia 2022 平台建设专场的分享，本篇内容主要分为四个部分：</p><ul><li>实时计算平台背景介绍</li><li>Flink 实时作业运维挑战</li><li>基于 StreamPark 一体化管理</li><li>未来规划与演进</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="实时计算平台背景介绍"><strong>实时计算平台背景介绍</strong><a href="#实时计算平台背景介绍" class="hash-link" aria-label="Direct link to 实时计算平台背景介绍" title="Direct link to 实时计算平台背景介绍">​</a></h2><p><img loading="lazy" src="/assets/images/overall_architecture-efc4272dcb2fcdadac42d40cf1a6a931.png" width="1080" height="597" class="img_ev3q"></p><p>上图是实时计算平台的整体架构，最底层是数据源，由于一些敏感信息，没有将数据源的详细信息列出，它主要包含三部分，分别是业务数据库、用户行为日志、用户位置，联通的数据源非常多，业务数据库这一项就有几万张表；主要通过 Flink SQL 和 DataStream API 来处理数据 ，数据处理流程包括 Flink 对数据源的实时解析、规则的实时计算以及实时产品；用户在可视化订阅平台上进行实时数据订阅，用户可以在地图上画一个电子围栏，并设置一些规则，如来自于哪里，在围栏里驻留多长时间等，还可以筛选一些特征，符合这些规则的用户信息会实时进行推送，然后是实时安全部分，当某个用户连接了高危基站或是有异常操作行为时，我们会认为可能存在诈骗行为，会对手机号码进行关停等等，还有用户的一些实时特征以及实时大屏。</p><p><img loading="lazy" src="/assets/images/data_processing_processes-6b611077be80fd421d883accd5acb423.png" width="1080" height="593" class="img_ev3q"></p><p>上图是数据处理的详细的流程。</p><p>第一部分是采集解析，我们的数据源来自于业务的数据库，包含 OGG 和 DTS 格式的消息、日志消息、用户行为和用户位置数据，总共 50 多种数据源，后续还会逐渐增加，所有数据源均使用 Flink 做实时解析；并增加了 Metrics 来监控数据源的延迟情况。</p><p>第二部分是实时计算， 这个环节处理的数据量很大，数据量在万亿级别，支撑了 10000+的数据实时订阅，有 200 多个 Flink 任务，我们将某一种同类型的业务封装成一种场景，同一个 Flink 作业可以支持相同场景的多个订阅，目前 Flink 作业数还在不停的增长，后续可能会增加到 500 多个；其中面临的一个很大挑战是每天万亿级的数据实时关联电子围栏、用户特征等信息，电子围栏有几万个，用户特征涉及数亿用户，最初我们将电子围栏信息和用户特征放到 HBase, 但这样会导致 HBase 压力很大，经常遇到性能问题造成数据延迟，而且一旦产生数据积压，需要很长的时间去消化，得益于 Flink State 的强大，我们将电子围栏信息和用户特征放到状态里，目前已经很好的支撑了大并发的场景，同时我们也增加了数据处理的性能监控；最后是实时产品和营销触达前端的一些应用。</p><p><img loading="lazy" src="/assets/images/platform_evolution-9d3427a9bb13529e6a6540f6a77e152b.png" width="1080" height="483" class="img_ev3q"></p><p>2018 年采用了三方黑盒的计算引擎，不能支持灵活定制个性化功能，且依赖过多外部系统，导致外部系统负载高，运维复杂；2019 年使用了 Spark Streaming 的微批处理，2020 年开始使用 Flink 的流式计算，从 2021 年开始，几乎所有 Spark Streaming 的微批处理都被 Flink 替代了，同时上线了 Apache StreamPark 对我们的 Flink 作业进行管理。</p><p><img loading="lazy" src="/assets/images/platform_background-e591fd5ed3391480217c668f70519d28.png" width="1080" height="493" class="img_ev3q"></p><p>总结一下平台背景，主要包含以下几部分：</p><ul><li>数据量大：日均万亿的数据处理。</li><li>数据源多：集成了 50 多种实时数据源。</li><li>订阅多：支撑了 10000+的数据服务订阅。</li><li>用户多：支撑了 30 多个内部和外部用户使用。</li></ul><p><img loading="lazy" src="/assets/images/operational_background-dd9ae29be9c5d959a649bb9dd8f7a1dc.png" width="1080" height="562" class="img_ev3q"></p><p>运维背景也可以分为以下几部分：</p><ul><li>支撑需求多：50 多种数据源，10000+的数据服务订阅。</li><li>实时作业多：现在有 200+Flink 生产作业，并且持续快速增长中, 未来可达 500+。</li><li>上线频率高：每天都有新增的或增强的 Flink 作业上线操作。</li><li>开发人员多：50+研发人员参与开发 Flink 实时计算任务。</li><li>使用用户多：30+内部和外部组织的用户使用。</li><li>监控延迟低：一旦发现问题我们要立马进行处理，避免引起用户的投诉。</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="flink-实时作业运维挑战"><strong>Flink 实时作业运维挑战</strong><a href="#flink-实时作业运维挑战" class="hash-link" aria-label="Direct link to flink-实时作业运维挑战" title="Direct link to flink-实时作业运维挑战">​</a></h2><p><img loading="lazy" src="/assets/images/difficulties-81db2f256a9e39ae8c9ba9730aa37585.png" width="1080" height="481" class="img_ev3q"></p><p>基于平台和运维背景，尤其是 Flink 作业越来越多的情况下，遇到了很大的挑战，主要有两方面，分别是作业运维困境和业务支撑困境。</p><p>在作业运维困境上，首先作业部署流程长、效率低；在联通安全是第一红线下，在服务器上部署程序的时候，要连接 VPN、登录 4A、打包编译、部署、然后再启动，整个流程比较长，最初在开发 Flink 的时候，都是用脚本启动的，导致代码分支是不可控的，部署完之后也难以追溯，再就是脚本很难与 git 上的代码进行同步，因为对于脚本代码，开发人员更喜欢在服务器上直接改，很容易忘记上传 git。</p><p>由于作业运维困境上的种种因素，会产生业务支撑困境，如导致上线故障率高、影响数据质量、上线时间长、数据延迟高、告警漏发处理等，引起的投诉，此外，我们的业务影响不明确，一旦出现问题，处理问题会成为第一优先级。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="基于-streampark-一体化管理"><strong>基于 StreamPark 一体化管理</strong><a href="#基于-streampark-一体化管理" class="hash-link" aria-label="Direct link to 基于-streampark-一体化管理" title="Direct link to 基于-streampark-一体化管理">​</a></h2><p><img loading="lazy" src="/assets/images/job_management-57ac8eebee7ab32d4d55008faade15ff.png" width="1080" height="510" class="img_ev3q"></p><p>对于以上的两种困境，我们基于 StreamPark 一体化管理解决了很多问题，首先来看一下 StreamPark 的双线演进，分别是 Flink 作业管理和 Flink 作业 DevOps 平台；在作业管理上，StreamPark 支持将 Flink 实时作业部署到不同的集群里去，比如 Flink 原生自带的 Standalone 模式，Flink on Yarn 的 Session、Application、PerJob 模式，在最新的版本中将支持 Kubernetes Native Session 模式；中间层是项目管理、作业管理、集群管理、团队管理、变量管理、告警管理。</p><ul><li>项目管理：当部署 Flink 程序的时候，可以在项目管理里填写 git 地址，同时选择要部署的分支。</li><li>作业管理：可以指定 Flink 作业的执行模式，比如你要提交到什么类型的集群里去，同时还可以配置一些资源，比如 TaskManager 的数量、TaskManager/JobManager 的内存大小、并行度等等，还可以设置一些容错，比如 Flink 作业失败后，StreamPark 可以支持它自动拉起，同时支持传入一些动态参数。</li><li>集群管理：可以在界面上添加和管理大数据集群。</li><li>团队管理：在企业的实际生产过程中会有多个团队，团队之间是隔离的。</li><li>变量管理：可以把一些变量统一维护在一个地方，比如 Kafka 的 Broker 地址定义成一个变量，在配置 Flink 作业或者 SQL 的时候，就可以以变量的方式来替换 Broker 的 IP，且后续这个 Kafka 要下线的时候，也可以通过这个变量去查看到底哪些作业使用了这个集群，方便我们去做一些后续的流程。</li><li>告警管理：支持多种告警模式，如微信、钉钉、短信和邮件。</li></ul><p>StreamPark 支持 Flink SQL、Flink Jar 的提交，支持资源配置，支持状态跟踪，如状态是运行状态，失败状态等，同时支持指标大屏和各种日志查看。</p><p><img loading="lazy" src="/assets/images/devops_platform-97e53d17c155a4d98431a1ab52a7bd13.png" width="1080" height="501" class="img_ev3q"></p><p>Flink 作业 DevOps 平台，主要包括以下几部分：</p><ul><li>团队：StreamPark 支持多个团队，每个团队都有团队的管理员，他拥有所有权限，同时还有团队的开发者，他只有少量的一部分权限。</li><li>编译、打包：在创建 Flink 项目时，可以把 git 地址、分支、打包的命令等配置在项目里，然后一键点击 build 按钮进行编译、打包。</li><li>发布、部署：发布和部署的时候会创建 Flink 作业，在 Flink 作业里可以选择执行模式、部署集群、资源设置、容错设置、变量填充，最后通过一键启动停止，启动 Flink 作业。</li><li>状态监测：Flink 作业启动完成之后，就是状态的实时跟踪，包括 Flink 的运行状态、运行时长、Checkpoint 信息等，并支持一键跳转到 Flink 的 Web UI。</li><li>日志、告警：包含构建的一些日志和启动日志，同时支持钉钉、微信、邮件、短信等告警方式。</li></ul><p><img loading="lazy" src="/assets/images/multi_team_support-ea9b19ef43d528e9be9016efa65f80fc.png" width="1080" height="477" class="img_ev3q"></p><p>企业一般有多个团队同时开发实时作业，在我们公司包含实时采集团队、数据处理团队和实时的营销团队，StreamPark 支持多个团队的资源隔离。</p><p><img loading="lazy" src="/assets/images/platformized_management-f3f784103707c8e3b67a3bcdda5a3dd6.png" width="1080" height="600" class="img_ev3q"></p><p>Flink 作业平台化管理面临如下挑战：</p><ul><li>脚本数量多：平台有几百个脚本，分散在多个服务器上。</li><li>脚本类型多：在启动 Flink 作业时，会有启动脚本、停止脚本和守护脚本，而且操作权限很难控制。</li><li>脚本不一致：服务器上的脚本与 git 上的脚本不一致。</li><li>脚本确权难：Flink 作业的责任人，用途不明确。</li><li>分支不可控：启动作业的时候，需要在脚本里指定 git 分支，导致分支不可追溯的。</li></ul><p>基于以上的挑战，StreamPark 通过项目管理来解决了责任人不明确，分支不可追溯的问题，因为在创建项目的时候需要手动指定一些分支，一旦打包成功，这些分支是有记录的；通过作业管理对配置进行了集中化，避免了脚本太过于分散，而且作业启动、停止的权限有严格的控制，避免了脚本化权限不可控的状态，StreamPark 以接口的方式与集群进行交互来获取作业信息，这样做会让作业控制更加精细。</p><p>可以看一下上图中下面的图，通过项目管理进行打包，通过作业管理进行配置，然后发布，可以进行一键启停，通过 API 提交作业。</p><p><img loading="lazy" alt="图片" src="/assets/images/development_efficiency-2b27930ac18721c40acb006c03806e69.png" width="1080" height="591" class="img_ev3q"></p><p>早期我们需要通过 7 步进行部署，包括连接 VPN、登录 4A、执行编译脚本、执行启动脚本、打开 Yarn、搜索作业名、进入 Flink UI 等 7 个步骤，StreamPark 可以支持 4 个一键进行部署，包括一键打包、一键发布、一键启动、一键到 Flink UI。</p><p><img loading="lazy" alt="图片" src="/assets/images/submission_process-bb3efd39e3f2e76595995647327be75a.png" width="1080" height="548" class="img_ev3q"></p><p>上图是我们 StreamPark 的作业提交流程，首先 StreamPark 会将作业进行发布，发布的时候会上传一些资源，然后会进行作业的提交，提交的时候会带上配置的一些参数，以 Flink Submit 的方式调用接口发布到集群上；这里会有多个 Flink Submit 对应着不同的执行模式，比如 Yarn Session、Yarn Application、Kubernetes Session、Kubernetes Application 等都是在这里控制的，提交作业之后，如果是 Flink on Yarn 作业，会得到这个 Flink 作业的 Application ID 或者 Job ID，这个 ID 会保存在我们的数据库中，如果是基于 Kubernetes 执行的话，也会得到 Job ID，后面我们在跟踪作业状态的时候，主要就是通过保存的这些 ID 去跟踪作业的状态。</p><p><img loading="lazy" alt="图片" src="/assets/images/status_acquisition_bottleneck-436fee79261994fd26eda735d2833e70.png" width="1080" height="599" class="img_ev3q"></p><p>如上所述，如果是 Flink on Yarn 作业，在提交作业的时候会获取两个 ID，Application ID 或者 Job ID，基于这两个 ID 可以获取我们的状态，但当 Flink 作业非常多的时候会遇到一些问题，StreamPark 它是有一个状态获取器，它会通过我们保存的数据库里的 Application ID 或者 Job ID，去向 ResourceManager 做一个请求，会做每五秒钟周期性的轮询，如果作业特别多，每次轮询 ResourceManager 会负责再去调用 Job Manager 的地址访问它的状态，这就会导致 ResourceManager 的连接数压力较大和连接数过高。</p><p>上图中 ResourceManager 的连接数阶段性、周期性的持续走高，可以看到 ResourceManager 处于比较红的状态，从主机上去监控的时候，它的连接数确实比较高。</p><p><img loading="lazy" alt="图片" src="/assets/images/state_optimization-06c675fa387cb316cc225fb0b1576451.png" width="1080" height="596" class="img_ev3q"></p><p>针对上面的问题，我们做了一些优化，首先 StreamPark 保存了提交作业之后的 Application ID 或者 Job ID，同时也会获取 Job Manager 直接访问的地址，并保存在数据库中，每次轮询时不再通过 ResourceManager 获取作业的状态，它可以直接调用各个 Job Manager 的地址实时获取状态，极大的降低了 ResourceManager 的连接数；从上图最后的部分可以看到，基本不会产生太大的连接数，大大减轻了 ResourceManager 的压力，且后续当 Flink 作业越来越多时获取状态也不会遇到瓶颈的问题。</p><p><img loading="lazy" alt="图片" src="/assets/images/state_recovery-7d0a5ecaf3c3af7ffe33f85abf3d51a8.png" width="1080" height="573" class="img_ev3q"></p><p>StreamPark 解决的另一个问题是 Flink 从状态恢复的保障，以前我们用脚本做运维的时候，在启动 Flink 的时候，尤其是在业务升级的时候，要从上一个最新的 Checkpoint 来恢复，但经常有开发人员忘记从上一个检查点进行恢复，导致数据质量产生很大的问题，遭到投诉，StreamPark 的流程是在首次启动的时候，每五秒钟轮询一次获取 Checkpoint 的记录，同时保存在数据库之中，在 StreamPark 上手动停止 Flink 作业的时候，可以选择做不做 Savepoint，如果选择了做 Savepoint，会将 Savepoint 的路径保存在数据库中，同时每次的 Checkpoint 记录也保存在数据库中，当下次启动 Flink 作业的时候，默认会选择最新的 Checkpoint 或者 Savepoint 记录，有效避免了无法从上一个检查点去恢复的问题，也避免了导致问题后要进行 offset 回拨重跑作业造成的资源浪费，同时也保证了数据处理的一致性。</p><p><img loading="lazy" alt="图片" src="/assets/images/multiple_environments_and_components-85e9f3f218c9c5ef79d3792aec9540ac.png" width="1080" height="576" class="img_ev3q"></p><p>StreamPark 还解决了在多环境下多个组件的引用挑战，比如在企业中通常会有多套环境，如开发环境、测试环境、生产环境等，一般来说每套环境下都会有多个组件，比如 Kafka，HBase、Redis 等，而且在同一套环境里还可能会存在多个相同的组件，比如在联通的实时计算平台，从上游的 Kafka 消费数据的时候，将符合要求的数据再写到下游的 Kafka，这个时候同一套环境会涉及到两套 Kafka，单纯从 IP 很难判断是哪个环境哪个组件，所以我们将所有组件的 IP 地址都定义成一个变量，比如 Kafka 集群，开发环境、测试环境、生产环境都有 Kafka.cluster 这个变量，但它们指向的 Broker 的地址是不一样的，这样不管是在哪个环境下配置 Flink 作业，只要引用这个变量就可以了，大大降低了生产上的故障率。</p><p><img loading="lazy" alt="图片" src="/assets/images/multiple_execution_modes-56bb5cc794122dd7ac987f57999afb33.png" width="1080" height="585" class="img_ev3q"></p><p>StreamPark 支持 Flink 多执行的模式，包括基于 on Yarn 的 Application/ Perjob / Session 三种部署模式，还支持 Kubernetes 的 Application 和 Session 两种部署模式，还有一些 Remote 的模式。</p><p><img loading="lazy" alt="图片" src="/assets/images/versioning-641f09a5fcccd37dda2ed9d4cac14413.png" width="1080" height="389" class="img_ev3q"></p><p>StreamPark 也支持 Flink 的多版本，比如联通现在用的是 1.14.x，现在 1.16.x 出来后我们也想体验一下，但不可能把所有的作业都升级到 1.16.x，我们可以把新上线的升级到 1.16.x，这样可以很好的满足使用新版本的要求，同时也兼容老版本。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="未来规划与演进"><strong>未来规划与演进</strong><a href="#未来规划与演进" class="hash-link" aria-label="Direct link to 未来规划与演进" title="Direct link to 未来规划与演进">​</a></h2><p><img loading="lazy" alt="图片" src="/assets/images/contribution_and_enhancement-bbb20c198ecc048735b12e40157115e2.png" width="1080" height="418" class="img_ev3q"></p><p>未来我们将加大力度参与 StreamPark 建设，以下我们计划要增强的方向。</p><ul><li>高可用：StreamPark 目前不支持高可用，这方面还需要做一些加强。</li><li>状态的管理：在企业实践中 Flink 作业在上线时，每个算子会有 UID。如果 Flink UID 不做设置，做 Flink 作业的升级的时候，就有可能出现状态无法恢复的情况，目前通过平台还无法解决这个问题，所以我们想在平台上增加这个功能，在 Flink Jar 提交时，增加检测算子是否设置 UID 的功能，如果没有，会发出提醒，这样可以避免每次上线 Flink 作业时，作业无法恢复的问题；之前遇到这种情况的时候，我们需要使用状态处理的 API，从原来的状态里进行反序列化，然后再用状态处理 API 去制作新的状态，供升级后的 Flink 加载状态。</li><li>更细致的监控：目前支持 Flink 作业失败之后，StreamPark 发出告警。我们希望 Task 失败之后也可以发出告警，我们需要知道失败的原因；还有作业反压监控告警、Checkpoint 超时、失败告警性能指标采集，也有待加强。</li><li>流批一体：结合 Flink 流批一体引擎和数据湖流批一体存储探索流批一体平台。</li></ul><p><img loading="lazy" src="/assets/images/road_map-1a30ee719bdadf2e0ea31d6ea9e957bc.png" width="1080" height="488" class="img_ev3q"></p><p>上图是 StreamPark 的 Roadmap。</p><ul><li>数据源：StreamPark 会支持更多数据源的快速接入，达到数据一键入户。</li><li>运维中心：获取更多 Flink Metrics 进一步加强监控运维的能力。</li><li>K8S-operator：现有的 Flink on K8S 还是有点重，经历了打 Jar 包、打镜像、推镜像的过程，后续需要改进优化，积极拥抱上游对接的 K8S-operator。</li><li>流式数仓：增强对 Flink SQL 作业能力的支持，简化 Flink SQL 作业的提交，计划对接 Flink SQL Gateway；SQL 数仓方面的能力加强，包括元数据存储、统一建表语法校验、运行测试、交互式查询，积极拥抱 Flink 上游，探索实时数仓和流式数仓。</li></ul></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/stream-park">StreamPark</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/生产实践">生产实践</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/flink-sql">FlinkSQL</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="导读：本文主要介绍顺网科技在使用 Flink 计算引擎中遇到的一些挑战，基于 StreamPark 作为实时数据平台如何来解决这些问题，从而大规模支持公司的业务。"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/streampark-usercase-shunwang">StreamPark 在顺网科技的大规模生产实践</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-10-24T15:06:09.000Z" itemprop="datePublished">October 24, 2023</time> · <!-- -->19 min read</div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" src="/assets/images/autor-4fee09aa3abf8842dcdbb7ada1aa9409.png" width="1080" height="460" class="img_ev3q"></p><p><strong>导读：</strong>本文主要介绍顺网科技在使用 Flink 计算引擎中遇到的一些挑战，基于 StreamPark 作为实时数据平台如何来解决这些问题，从而大规模支持公司的业务。</p><ul><li>公司业务介绍</li><li>遇到的挑战</li><li>为什么用 StreamPark</li><li>落地实践</li><li>带来的收益</li><li>未来规划</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="公司业务介绍"><strong>公司业务介绍</strong><a href="#公司业务介绍" class="hash-link" aria-label="Direct link to 公司业务介绍" title="Direct link to 公司业务介绍">​</a></h2><p>杭州顺网科技股份有限公司成立于 2005 年，秉承科技连接快乐的企业使命，是国内具有影响力的泛娱乐技术服务平台之一。多年来公司始终以产品和技术为驱动，致力于以数字化平台服务为人们创造沉浸式的全场景娱乐体验。</p><p>自顺网科技成立以来，随着业务快速发展，顺网科技服务了 8 万家线下实体店，拥有超过 5000 万互联网用户，年触达超 1.4 亿网民，每 10 家公共上网服务场所有 7 家使用顺网科技产品。</p><p>在拥有庞大的用户群体的情况下，顺网科技为了给用户提供更加优质的产品体验，实现企业的数字化转型，从 2015 年开始大力发展大数据， Flink 在顺网科技的实时计算中一直扮演着重要的角色。在顺网科技，实时计算大概分为 4 个应用场景：</p><ul><li>用户画像实时更新：包括网吧画像和网民画像。</li><li>实时风控：包括活动防刷、异地登录监测等。</li><li>数据同步：包括 Kafka 数据同步到 Hive / Iceberg / ClickHouse 等。</li><li>实时数据分析：包括游戏、语音、广告、直播等业务实时大屏。</li></ul><p>到目前为止，顺网科技每日需要处理 TB 级别的数据，总共拥有 700+ 个实时任务，其中 FlinkSQL 任务占比为 95% 以上。随着公司业务快速发展和数据时效性要求变高，预计在今年年底 Flink 任务会达到 900+。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="遇到的挑战"><strong>遇到的挑战</strong><a href="#遇到的挑战" class="hash-link" aria-label="Direct link to 遇到的挑战" title="Direct link to 遇到的挑战">​</a></h2><p>Flink 作为当下实时计算领域中最流行的技术框架之一，拥有高吞吐、低延迟、有状态计算等强大的特性。在探索中我们发现 Flink 虽然拥有强大的计算能力，但是对于作业开发管理和运维问题，社区并没有提供有效的解决方案。我们对 Flink 作业开发管理上遇到的一些痛点大概总结为 4 个方面，如下：</p><p><img loading="lazy" alt="图片" src="/assets/images/pain-4d2aee42ea7aae80eaeac17a6d51c090.png" width="1080" height="744" class="img_ev3q"></p><p>在面对 Flink 作业管理和运维上的的一系列痛点后，我们一直在寻找合适的解决方案来降低开发同学使用 Flink 门槛，提高工作效率。</p><p>在没有遇到 StreamPark 之前，我们调研了部分公司的 Flink 管理解决方案，发现都是通过自研实时作业平台的方式来开发和管理 Flink 作业。于是，我们也决定自研一套实时计算管理平台，来满足了开发同学对于 Flink 作业管理和运维的基础需求，我们这套平台叫 Streaming-Launcher，大体功能如下：</p><p><img loading="lazy" alt="图片" src="/assets/images/launcher-766611cb0dddb99d8437ead44d0665d4.png" width="1080" height="670" class="img_ev3q"></p><p>但是后续开发同学在使用过程中，发现 Streaming-Launcher 存在比较多的缺陷：Flink 开发成本依然过高、工作效率低下、问题排查困难。我们总结了 Streaming-Launcher 存在的缺陷，大致如下：</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="sql开发流程繁琐"><strong>SQL开发流程繁琐</strong><a href="#sql开发流程繁琐" class="hash-link" aria-label="Direct link to sql开发流程繁琐" title="Direct link to sql开发流程繁琐">​</a></h3><p>作业务开发需要多个工具完成一个 SQL 作业开发，提高了开发同学的使用门槛。</p><p><img loading="lazy" alt="cc0b1414ed43942e0ef5e9129c2bf817" src="/assets/images/sql_develop-ef85f6a001fb814d494c9b39eb2dfba9.png" width="1080" height="229" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="sql-client-存在弊端"><strong>SQL-Client 存在弊端</strong><a href="#sql-client-存在弊端" class="hash-link" aria-label="Direct link to sql-client-存在弊端" title="Direct link to sql-client-存在弊端">​</a></h3><p>Flink 提供的 SQL-Client 目前对作业运行模式支持上，存在一定的弊端。</p><p><img loading="lazy" alt="图片" src="/assets/images/sql_client-32fd9eb063198e6f35349187eed61546.png" width="1080" height="721" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="作业缺少统一管理"><strong>作业缺少统一管理</strong><a href="#作业缺少统一管理" class="hash-link" aria-label="Direct link to 作业缺少统一管理" title="Direct link to 作业缺少统一管理">​</a></h3><p>Streaming-Launcher 中，没有提供统一的作业管理界面。开发同学无法直观的看到作业运行情况，只能通过告警信息来判断作业运行情况，这对开发同学来说非常不友好。如果因为 Yarn 集群稳定性问题或者网络波动等不确定因素，一下子失败大批量任务，在开发同学手动恢复作业的过程中，很容易漏恢复某个任务而造成生产事故。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="问题诊断流程繁琐"><strong>问题诊断流程繁琐</strong><a href="#问题诊断流程繁琐" class="hash-link" aria-label="Direct link to 问题诊断流程繁琐" title="Direct link to 问题诊断流程繁琐">​</a></h3><p>一个作业查看日志需要通过多个步骤，一定程度上降低了开发同学工作效率。</p><p><img loading="lazy" alt="图片" src="/assets/images/step-d945592bbe32c0e093289c6265472f1c.png" width="1080" height="122" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="为什么用-streampark"><strong>为什么用</strong> <strong>StreamPark</strong><a href="#为什么用-streampark" class="hash-link" aria-label="Direct link to 为什么用-streampark" title="Direct link to 为什么用-streampark">​</a></h2><p>面对自研平台 Streaming-Launcher 存在的缺陷，我们一直在思考如何将 Flink 的使用门槛降到更低，进一步提高工作效率。考虑到人员投入成本和时间成本，我们决定向开源社区求助寻找合适的开源项目来对我们的 Flink 任务进行管理和运维。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="01--streampark-解决-flink-问题的利器">01  <strong>StreamPark 解决 Flink 问题的利器</strong><a href="#01--streampark-解决-flink-问题的利器" class="hash-link" aria-label="Direct link to 01--streampark-解决-flink-问题的利器" title="Direct link to 01--streampark-解决-flink-问题的利器">​</a></h3><p>很幸运在 2022 年 6 月初，我们在 GitHub 机缘巧合之间认识到了 StreamPark，我们满怀希望地对 StreamPark 进行了初步的探索。发现 StreamPark 具备的能力大概分为三大块：用户权限管理、作业运维管理和开发脚手架。</p><p><strong>用户权限管理</strong></p><p>在 StreamPark 平台中为了避免用户权限过大，发生一些不必要的误操作，影响作业运行稳定性和环境配置的准确性，提供了相应的一些用户权限管理功能，这对企业级用户来说，非常有必要。</p><p><img loading="lazy" alt="图片" src="/assets/images/permission-affd274acd10773ad4531da9102f3f91.png" width="1080" height="721" class="img_ev3q"></p><p><strong>作业运维管理</strong></p><p><strong>我们在对 StreamPark 做调研的时候，最关注的是 StreamPark 对于作业的管理的能力。</strong>StreamPark 是否有能力管理作业一个完整的生命周期：作业开发、作业部署、作业管理、问题诊断等。<strong>很幸运，StreamPark 在这一方面非常优秀，对于开发同学来说只需要关注业务本身，不再需要特别关心 Flink 作业管理和运维上遇到的一系列痛点。</strong>在 StreamPark 作业开发管理管理中，大致分为三个模块：作业管理基础功能，Jar 作业管理，FlinkSQL 作业管理。如下：</p><p><img loading="lazy" alt="图片" src="/assets/images/homework_manager-96dc37860dd87f12ed55311b38116931.png" width="1080" height="542" class="img_ev3q"></p><p><strong>开发脚手架</strong></p><p>通过进一步的研究发现，StreamPark 不仅仅是一个平台，还包含 Flink 作业开发脚手架, 在 StreamPark 中，针对编写代码的 Flink 作业，提供一种更好的解决方案，将程序配置标准化，提供了更为简单的编程模型，同时还提供了一些列 Connectors，降低了 DataStream 开发的门槛。</p><p><img loading="lazy" alt="图片" src="/assets/images/connectors-d405c391d0cfdd753e7b329433275117.png" width="1080" height="720" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="02--streampark-解决自研平台的问题">02  <strong>StreamPark 解决自研平台的问题</strong><a href="#02--streampark-解决自研平台的问题" class="hash-link" aria-label="Direct link to 02--streampark-解决自研平台的问题" title="Direct link to 02--streampark-解决自研平台的问题">​</a></h3><p>上面我们简单介绍了 StreamPark 的核心能力。在顺网科技的技术选型过程中，我们发现 StreamPark 所具备强大的功能不仅包含了现有 Streaming-Launcher 的基础功能，还提供了更完整的对应方案解决了 Streaming-Launcher 的诸多不足。在这部分，着重介绍下 StreamPark 针对我们自研平台 Streaming-Launcher 的不足所提供的解决方案。</p><p><img loading="lazy" alt="图片" src="/assets/images/function-c2d3ad419e1676a0253d933935921a71.png" width="1080" height="720" class="img_ev3q"></p><p><strong>Flink 作业一站式的开发能力</strong></p><p>StreamPark 为了降低 Flink 作业开发门槛，提高开发同学工作效率，<strong>提供了 FlinkSQL IDE、参数管理、任务管理、代码管理、一键编译、一键作业上下线等使用的功能</strong>。在调研中，我们发现 StreamPark 集成的这些功能可以进一步提升开发同学的工作效率。在某种程度上来说，开发同学不需要去关心 Flink 作业管理和运维的难题，只要专注于业务的开发。同时，这些功能也解决了 Streaming-Launcher 中 SQL 开发流程繁琐的痛点。</p><p><img loading="lazy" alt="图片" src="/assets/images/application-c99d4b0512fdb7d214e5429769629930.png" width="1080" height="959" class="img_ev3q"></p><p><strong>支持多种部署模式</strong></p><p>在 Streaming-Launcher 中，由于只支持 Yarn Session 模式，对于开发同学来说，其实非常不灵活。StreamPark 对于这一方面也提供了完善的解决方案。<strong>StreamPark 完整的支持了Flink 的所有部署模式：Remote、Yarn Per-Job、Yarn Application、Yarn Session、K8s Session、K8s Application**</strong>，<strong>可以让开发同学针对不同的业务场景自由选择合适的运行模式。</strong></p><p><strong>作业统一管理中心</strong></p><p>对于开发同学来说，作业运行状态是他们最关心的内容之一。在 Streaming-Launcher 中由于缺乏作业统一管理界面，开发同学只能通过告警信息和 Yarn 中Application 的状态信息来判断任务状态，这对开发同学来说非常不友好。StreamPark 针对这一点，提供了作业统一管理界面，可以一目了然查看到每个任务的运行情况。</p><p><img loading="lazy" alt="图片" src="/assets/images/management-9eb1dec5b2fa480e897cf1c12d1425d8.png" width="1080" height="572" class="img_ev3q"></p><p>在 Streaming-Launcher 中，开发同学在作业问题诊断的时候，需要通过多个步骤才能定位作业运行日志。StreamPark 提供了一键跳转功能，能快速定位到作业运行日志。</p><p><img loading="lazy" alt="图片" src="/assets/images/logs-e4e9f2084f1fbcf0a4afe079433cef0a.png" width="1080" height="575" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="落-地-实-践">落 地 实 践<a href="#落-地-实-践" class="hash-link" aria-label="Direct link to 落 地 实 践" title="Direct link to 落 地 实 践">​</a></h2><p>在 StreamPark 引入顺网科技时，由于公司业务的特点和开发同学的一些定制化需求，我们对 StreamPark 的功能做了一些增加和优化，同时也总结了一些在使用过程中遇到的问题和对应的解决方案。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="01--结合-flink-sql-gateway-的能力">01  <strong>结合 Flink-SQL-Gateway 的能力</strong><a href="#01--结合-flink-sql-gateway-的能力" class="hash-link" aria-label="Direct link to 01--结合-flink-sql-gateway-的能力" title="Direct link to 01--结合-flink-sql-gateway-的能力">​</a></h3><p>在顺网科技，我们基于 Flink-SQL-Gateway 自研了 ODPS 平台来方便业务开发同学管理 Flink 表的元数据。业务开发同学在 ODPS 上对 Flink 表进行 DDL 操作，然后在 StreamPark 上对创建的 Flink 表进行分析查询操作。在整个业务开发流程上，我们对 Flink 表的创建和分析实现了解耦，让开发流程显得比较清晰。</p><p>开发同学如果想在 ODPS 上查询实时数据，我们需要提供一个 Flink SQL 的运行环境。我们使用 StreamPark 运行了一个 Yarn Session 的 Flink 环境提供给 ODPS 做实时查询。</p><p><img loading="lazy" alt="图片" src="/assets/images/homework-96030dec4ea9db88b42668873f5a176d.png" width="1080" height="541" class="img_ev3q"></p><p>目前 StreamPark 社区为了进一步降低实时作业开发门槛，正在对接 Flink-SQL-Gateway。</p><p><a href="https://github.com/apache/streampark/issues/2274" target="_blank" rel="noopener noreferrer">https://github.com/apache/streampark/issues/2274</a></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="02--增强-flink-集群管理能力">02  <strong>增强 Flink 集群管理能力</strong><a href="#02--增强-flink-集群管理能力" class="hash-link" aria-label="Direct link to 02--增强-flink-集群管理能力" title="Direct link to 02--增强-flink-集群管理能力">​</a></h3><p>在顺网科技，存在大量从 Kafka 数据同步到 Iceberg / PG / Clickhouse / Hive 的作业。这些作业需要的 Yarn 对于资源要求和时效性要求不高，但是如果全部使用 Yarn Application 和 per-job 模式，每个任务都会启动 JobManager，那么会造成 Yarn 资源的浪费。对此，我们决定使用 Yarn Session 模式运行这些大量的数据同步作业。</p><p>在实践中我们发现业务开发同学很难直观的知道在每个 Yarn Session 中运行了多少个作业，其中包括作业总数和正在运行中的作业数量。基于这个原因，我们为了方便开发同学可以直观地观察到 Yarn Session 中的作业数量，在 Flink Cluster 界面增加了 All Jobs 和 Running Jobs 来表示在一个 Yarn Session 中总的作业数和正在运行的作业数。</p><p><img loading="lazy" alt="图片" src="/assets/images/cluster-96bd791c1c4f3d6144fbd1d134d89cd6.png" width="1080" height="543" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="03--增强告警能力">03  <strong>增强告警能力</strong><a href="#03--增强告警能力" class="hash-link" aria-label="Direct link to 03--增强告警能力" title="Direct link to 03--增强告警能力">​</a></h3><p>因为每个公司的短信告警平台实现都各不相同，所以 StreamPark 社区并没有抽象出统一的短信告警功能。在此，我们通过 Webhook 的方式，自己实现了短信告警功能。</p><p><img loading="lazy" alt="图片" src="/assets/images/alarm-8fa330c6a2d27cfd62878dcd20cd524c.png" width="1080" height="601" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="04--增加阻塞队列解决限流问题">04  <strong>增加阻塞队列解决限流问题</strong><a href="#04--增加阻塞队列解决限流问题" class="hash-link" aria-label="Direct link to 04--增加阻塞队列解决限流问题" title="Direct link to 04--增加阻塞队列解决限流问题">​</a></h3><p>在生产实践中，我们发现在大批量任务同时失败的时候，比如 Yarn Session 集群挂了，飞书 / 微信等平台在多线程同时调用告警接口时会存在限流的问题，那么大量的告警信息因为飞书 / 微信等平台限流问题，StreamPark 只会发送一部分的告警信息，这样非常容易误导开发同学排查问题。在顺网科技，我们增加了一个阻塞队列和一个告警线程，来解决限流问题。</p><p><img loading="lazy" alt="图片" src="/assets/images/queue-83552f1862acff8811b74488e8fe7c05.png" width="1080" height="320" class="img_ev3q"></p><p>当作业监控调度器检测到作业异常时，会产生一条作业异常的消息发送的阻塞队列中，在告警线程中会一直消费阻塞队列中的消息，在得到作业异常消息后则会根据用户配置的告警信息单线程发送到不同的平台中。虽然这样做可能会让用户延迟收到告警，但是我们在实践中发现同时有 100+ 个 Flink 作业失败，用户接受到告警的延迟时间小于 3s。对于这种延迟时间，我们业务开发同学完全是可以接受的。该改进目前已经记录 ISSUE，正在考虑贡献到社区中。</p><p><a href="https://github.com/apache/streampark/issues/2142" target="_blank" rel="noopener noreferrer">https://github.com/apache/streampark/issues/2142</a></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="带来的收益">带来的收益<a href="#带来的收益" class="hash-link" aria-label="Direct link to 带来的收益" title="Direct link to 带来的收益">​</a></h2><p>我们从 StreamX 1.2.3（StreamPark 前身）开始探索和使用，经过一年多时间的磨合，我们发现 StreamPark 真实解决了 Flink 作业在开发管理和运维上的诸多痛点。</p><p>StreamPark 给顺网科技带来的最大的收益就是降低了 Flink 的使用门槛，提升了开发效率。我们业务开发同学在原先的 Streaming-Launcher 中需要使用 vscode、GitLab 和调度平台等多个工具完成一个 FlinkSQL 作业开发，从开发到编译到发布的流程中经过多个工具使用，流程繁琐。StreamPark 提供一站式服务，可以在 StreamPark 上完成作业开发编译发布，简化了整个开发流程。</p><p><strong>目前 StreamPark 在顺网科技已经大规模在生产环境投入使用，StreamPark 从最开始管理的 500+ 个 FlinkSQL 作业增加到了近 700 个 FlinkSQL作业，同时管理了 10+ 个 Yarn Sesssion Cluster。</strong></p><p><img loading="lazy" alt="图片" src="/assets/images/achievements1-9eb1dec5b2fa480e897cf1c12d1425d8.png" width="1080" height="572" class="img_ev3q"></p><p><img loading="lazy" alt="图片" src="/assets/images/achievements2-5aa9b3c14892f9b1d8b054e82f3b4ad3.png" width="1080" height="545" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="未-来-规-划">未 来 规 划<a href="#未-来-规-划" class="hash-link" aria-label="Direct link to 未 来 规 划" title="Direct link to 未 来 规 划">​</a></h2><p>顺网科技作为 StreamPark 早期的用户之一，在 1 年期间内一直和社区同学保持交流，参与 StreamPark 的稳定性打磨，我们将生产运维中遇到的 Bug 和新的 Feature 提交给了社区。在未来，我们希望可以在 StreamPark 上管理 Flink 表的元数据信息，基于 Flink 引擎通过多 Catalog 实现跨数据源查询分析功能。目前 StreamPark 正在对接 Flink-SQL-Gateway 能力，这一块在未来对于表元数据的管理和跨数据源查询功能会提供了很大的帮助。</p><p>由于在顺网科技多是已 Yarn Session 模式运行的作业，我们希望 StreamPark 可以提供更多对于 Remote集群、Yarn Session 集群和 K8s Session 集群功能支持，比如监控告警，优化操作流程等方面。</p><p>考虑到未来，随着业务发展可能会使用 StreamPark 管理更多的 Flink 实时作业，单节点模式下的 StreamPark 可能并不安全。所以我们对于 StreamPark 的 HA 也是非常期待。</p><p>对于 StreamPark 对接 Flink-SQL-Gateway 能力、丰富 Flink Cluster 功能和 StreamPark HA，我们后续也会参与建设中。</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/stream-park">StreamPark</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/生产实践">生产实践</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/flink-sql">FlinkSQL</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="摘要：本文源自 StreamPark 在尘锋信息的生产实践, 作者是资深数据开发工程师Gump。主要内容为："><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/streampark-usercase-dustess">StreamPark 在尘锋信息的最佳实践，化繁为简极致体验</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-10-24T15:06:09.000Z" itemprop="datePublished">October 24, 2023</time> · <!-- -->22 min read</div></header><div class="markdown" itemprop="articleBody"><p><strong>摘要：</strong>本文源自 StreamPark 在尘锋信息的生产实践, 作者是资深数据开发工程师Gump。主要内容为：</p><ol><li>技术选型</li><li>落地实践</li><li>业务支撑 &amp; 能力开放</li><li>未来规划</li><li>结束语</li></ol><p>尘锋信息是基于企业微信生态的一站式私域运营管理解决方案供应商，致力于成为全行业首席私域运营与管理专家，帮助企业构建数字时代私域运营管理新模式，助力企业实现高质量发展。</p><p>目前，尘锋已在全国拥有13个城市中心，覆盖华北、华中、华东、华南、西南五大区域，为超30个行业的10,000+家企业提供数字营销服务。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="01-技术选型"><strong>01 技术选型</strong><a href="#01-技术选型" class="hash-link" aria-label="Direct link to 01-技术选型" title="Direct link to 01-技术选型">​</a></h2><p>尘锋信息在2021年进入了快速发展时期，随着服务行业和企业客户的增加，实时需求越来越多，落地实时计算平台迫在眉睫。</p><p>由于公司处于高速发展期，需求紧迫且变化快，所以团队的技术选型遵循以下原则:</p><ul><li>快：由于业务紧迫，我们需要快速落地规划的技术选型并运用生产</li><li>稳：满足快的基础上，所选择技术一定要稳定服务业务</li><li>新：在以上基础，所选择的技术也尽量的新  </li><li>全：所选择技术能够满足公司快速发展和变化的业务，能够符合团队长期发展目标，能够支持且快速支持二次开发</li></ul><p>首先在计算引擎方面：我们选择 Flink，原因如下:</p><ul><li>团队成员对 Flink 有深入了解，熟读源码</li><li>Flink 支持批流一体，虽然目前公司的批处理架构还是基于 Hive、Spark 等。使用 Flink 进行流计算，便于后期建设批流一体和湖仓一体</li><li>目前国内 Flink 生态已经越来越成熟，Flink 也开始着手踏破边界向流式数仓发展</li></ul><p>在平台层面，我们综合对比了 StreamPark 、 Apache Zeppelin 和 flink-streaming-platform-web，也深入阅读了源码和并做了优缺点分析，关于后两个项目本文就不展开赘述，感兴趣的朋友可以去 GitHub 搜索，我们最终选择 StreamPark，理由如下：</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="完成度高"><strong>完成度高</strong><a href="#完成度高" class="hash-link" aria-label="Direct link to 完成度高" title="Direct link to 完成度高">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="1-支持flink-多版本"><strong>1. 支持Flink 多版本</strong><a href="#1-支持flink-多版本" class="hash-link" aria-label="Direct link to 1-支持flink-多版本" title="Direct link to 1-支持flink-多版本">​</a></h4><p>//视频链接（ Flink 多版本支持 Demo ）</p><p>新建任务时可以<strong>自由选择Flink版本</strong>，Flink 二进制版本包会自动上传至 HDFS（如果使用 Yarn 提交），且一个版本的二进制包只会在 HDFS 保存一份。任务启动时会自动根据上下文加载 HDFS 中的 Flink 二进制包，非常优雅。能够满足多版本共存，及升级Flink 新版本试用测试的场景。</p><p><img loading="lazy" src="/assets/images/flink_home-0a6f4f2014cc87b074ef259088af2b98.png" width="1080" height="223" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="2-支持多种部署模式"><strong>2. 支持多种部署模式</strong><a href="#2-支持多种部署模式" class="hash-link" aria-label="Direct link to 2-支持多种部署模式" title="Direct link to 2-支持多种部署模式">​</a></h4><p>StreamPark 支持 Flink <strong>所有主流的提交模式</strong>，如 standalone、yarn-session 、yarn application、yarn-perjob、kubernetes-session、kubernetes-application  而且StreamPark 不是简单的拼接 Flink run 命令来进行的任务提交，而是引入了 Flink Client 源码包，直接调用 Flink Client API 来进行的任务提交。这样的好处是代码模块化、易读、便于扩展，稳定，且能在后期根据 Flink 版本升级进行很快的适配。</p><p><img loading="lazy" src="/assets/images/execution_mode-1182aeb8efe9572ec98c2a2b95293dc1.png" width="1080" height="324" class="img_ev3q"></p><p>Flink SQL 可以极大提升开发效率和提高 Flink 的普及。StreamPark 对于 <strong>Flink SQL 的支持非常到位</strong>，优秀的 SQL 编辑器，依赖管理，任务多版本管理等等。StreamPark 官网介绍后期会加入 Flink SQL 的元数据管理整合，大家拭目以待。</p><p><img loading="lazy" src="/assets/images/flink_sql-13f6952f92585140c5fc640b490918b0.png" width="1080" height="779" class="img_ev3q"></p><p><img loading="lazy" src="/assets/images/flink_sql_version-a02b0f0eac9c5d6c0281b7471e438b78.png" width="1080" height="736" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="4-java任务在线构建"><strong>4. JAVA任务在线构建</strong><a href="#4-java任务在线构建" class="hash-link" aria-label="Direct link to 4-java任务在线构建" title="Direct link to 4-java任务在线构建">​</a></h4><p>//视频链接（ JAVA 任务构建 Demo）</p><p>Flink SQL 现在虽然足够强大，但使用 Java 和 Scala 等 JVM 语言开发 Flink 任务会更加灵活、定制化更强、便于调优和提升资源利用率。与 SQL 相比 Jar 包提交任务最大的问题是Jar包的上传管理等，没有优秀的工具产品会严重降低开发效率和加大维护成本。 </p><p>StreamPark 除了支持 Jar 上传，更提供了<strong>在线更新构建</strong>的功能，优雅解决了以上问题：  </p><p>1、新建 Project ：填写 GitHub/Gitlab（支持企业私服）地址及用户名密码, StreamPark 就能 Pull 和 Build 项目。</p><p>2、创建 StreamPark Custom-Code 任务时引用 Project，指定主类，启动任务时可选自动 Pull、Build 和绑定生成的 Jar，非常优雅！</p><p>同时 StreamPark 社区最近也在完善整个任务编译、上线的流程，以后的 StreamPark 会在此基础上更加完善和专业。</p><p><img loading="lazy" src="/assets/images/system_list-6e9c13318d5aa2cfa4cdc11ac42c5844.png" width="1080" height="362" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="5-完善的任务参数配置"><strong>5. 完善的任务参数配置</strong><a href="#5-完善的任务参数配置" class="hash-link" aria-label="Direct link to 5-完善的任务参数配置" title="Direct link to 5-完善的任务参数配置">​</a></h4><p>对于使用 Flink 做数据开发而言，Flink run 提交的参数几乎是难以维护的。StreamPark 也非常<strong>优雅的解决</strong>了此类问题，原因是上面提到的 StreamPark 直接调用 Flink Client API，并且从 StreamPark 产品前端打通了整个流程。</p><p><img loading="lazy" src="/assets/images/parameter_configuration-94fb5d7ee0c9c04ed6d79ddb1cd3c1c7.png" width="1080" height="1001" class="img_ev3q"></p><p>大家可以看到，StreamPark 的任务参数设置涵盖了主流的所有参数，并且非常细心的对每个参数都做了介绍和最佳实践的最优推荐。这对于刚使用 Flink 的同学来说也是非常好的事情，避免踩坑！</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="6-优秀的配置文件设计"><strong>6. 优秀的配置文件设计</strong><a href="#6-优秀的配置文件设计" class="hash-link" aria-label="Direct link to 6-优秀的配置文件设计" title="Direct link to 6-优秀的配置文件设计">​</a></h4><p>对于 Flink 任务的原生参数，上面的任务参数已经涵盖了很大一部分。StreamPark 还提供了强大的<strong>Yaml 配置文件</strong> 模式和 <strong>编程模型</strong>。</p><p><img loading="lazy" src="/assets/images/extended_parameters-75c3f87809d0675c2fc82bc8d2ec096e.jpg" width="1080" height="2245" class="img_ev3q"></p><p>1、对于 Flink SQL 任务，直接使用任务的 Yaml 配置文件可以配置 StreamPark 已经内置的参数，如常见的 <strong>CheckPoint、重试机制、State Backend、table planer 、mode</strong> 等等。</p><p>2、对于 Jar 任务，StreamPark 提供了通用的编程模型，该模型封装了 Flink 原生 API ，结合 StreamPark 提供的封装包可以非常优雅的获取配置文件中的自定义参数，这块文档详见：</p><p>编程模型：</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">http://www.streamxhub.com/docs/development/dev-model</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>内置配置文件参数：</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">http://www.streamxhub.com/docs/development/config</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>除此之外：</p><p>StreamPark 也<strong>支持Apache Flink 原生任务</strong>，参数配置可以由 Java 任务内部代码静态维护，可以覆盖非常多的场景，比如存量 Flink 任务无缝迁移等等</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="7-checkpoint-管理"><strong>7. Checkpoint 管理</strong><a href="#7-checkpoint-管理" class="hash-link" aria-label="Direct link to 7-checkpoint-管理" title="Direct link to 7-checkpoint-管理">​</a></h4><p>关于 Flink 的 Checkpoint（Savepoint）机制，最大的困难是维护 ，StreamPark 也非常优雅的解决此问题:</p><ul><li>StreamPark 会<strong>自动维护</strong>任务 Checkpoint 的目录及版本至系统中方便检索</li><li>当用户需要更新重启应用时，可以选择是否保存 Savepoint</li><li>重新启动任务时可选择 Checkpoint/Savepoint 从指定版本的恢复</li></ul><p>如下，开发同学能够非常直观方便的升级或处理异常任务，非常强大</p><p><img loading="lazy" src="/assets/images/checkpoint-e9edd22da076247770a0b40595626fb7.png" width="1080" height="483" class="img_ev3q"></p><p><img loading="lazy" src="/assets/images/recover-f1de53fdbe66c50b465d58d0a66050de.jpg" width="1053" height="391" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="8-完善的报警功能"><strong>8. 完善的报警功能</strong><a href="#8-完善的报警功能" class="hash-link" aria-label="Direct link to 8-完善的报警功能" title="Direct link to 8-完善的报警功能">​</a></h4><p>对于流式计算此类7*24H常驻任务来说，监控报警是非常重要的 ，StreamPark 对于此类问题也有<strong>完善的解决方案</strong>:</p><ul><li>自带基于邮件的报警方式，0开发成本，配置即可使用</li><li>得益于 StreamPark 源码优秀的模块化，可以在 Task Track 处进行代码增强，引入公司内部的 SDK 进行电话、群组等报警方式，开发成本也非常低</li></ul><p><img loading="lazy" src="/assets/images/alarm_email-fd4c9ba1995ec69b7557bd1378dce737.png" width="1009" height="1340" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="源码优秀"><strong>源码优秀</strong><a href="#源码优秀" class="hash-link" aria-label="Direct link to 源码优秀" title="Direct link to 源码优秀">​</a></h3><p>遵循技术选型原则，一个新的技术必须足够了解底层原理和架构思想后，才会考虑应用生产。在选择 StreamPark 之前，对其架构和源码进入过深入研究和阅读。发现 StreamPark 所选用的底层技术是国人非常熟悉的：MySQL、Spring Boot、Mybatis Plus、Vue 等，代码风格统一，实现优雅，注释完善，各模块独立抽象合理，使用了较多的设计模式，且代码质量很高，非常适合后期的排错及二次开发。</p><p><img loading="lazy" src="/assets/images/code_notebook-542046feb8a312b5f6c057af551421c6.png" width="1080" height="527" class="img_ev3q"></p><p>StreamPark 于 2021年11月成功被开源中国评选为GVP - Gitee「最有价值开源项目」，足以见得其质量和潜力。</p><p><img loading="lazy" src="/assets/images/certificate-2f5b95ebb0816ead327ec169c12996b6.png" width="1080" height="684" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="03-社区活跃"><strong>03 社区活跃</strong><a href="#03-社区活跃" class="hash-link" aria-label="Direct link to 03-社区活跃" title="Direct link to 03-社区活跃">​</a></h3><p>目前社区非常活跃，从2021年11月底落地 StreamPark (基于1.2.0-release），当时StreamPark 刚刚才被大家认识，还有一些体验上的小 Bug（不影响核心功能）。当时为了快速上线，屏蔽掉了一些功能和修复了一些小 Bug，正当准备贡献给社区时发现早已修复，这也可以看出目前社区的迭代周期非常快。以后我们公司团队也会努力和社区保持一致，将新特性快速落地，提升数据开发效率和降低维护成本。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="02-落地实践"><strong>02 落地实践</strong><a href="#02-落地实践" class="hash-link" aria-label="Direct link to 02-落地实践" title="Direct link to 02-落地实践">​</a></h2><p>StreamPark 的环境搭建非常简单，跟随官网的搭建教程可以在小时内完成搭建。目前已经支持了前后端分离打包部署的模式，可以满足更多公司的需求，而且已经有 Docker Build 相关的 PR，相信以后 StreamPark 的编译部署会更加方便快捷。相关文档如下:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">http://www.streamxhub.com/docs/user-guide/deployment</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>为了快速落地和生产使用，我们选择了稳妥的 On Yarn 资源管理模式（虽然 StreamPark 已经很完善的支持 K8S），且已经有较多公司通过 StreamPark 落地了 K8S 部署方式，大家可以参考: </p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">http://www.streamxhub.com/blog/flink-development-framework-streamx</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>StreamPark 整合 Hadoop 生态可以说是0成本的（前提是按照 Flink 官网将 Flink 与 Hadoop 生态整合，能够通过 Flink 脚本启动任务即可）</p><p>目前我们也正在进行 K8S 的测试及方案设计，在未来一段时间会整体迁移至 K8S</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="01-落地-flinksql-任务"><strong>01 落地 FlinkSQL 任务</strong><a href="#01-落地-flinksql-任务" class="hash-link" aria-label="Direct link to 01-落地-flinksql-任务" title="Direct link to 01-落地-flinksql-任务">​</a></h3><p>目前我们公司基于 Flink SQL 的任务主要为业务比较简单的实时 ETL 和计算场景，数量在10个左右，上线至今都十分稳定。</p><p><img loading="lazy" src="/assets/images/online_flinksql-ca82a6f42e04e54e9f6da2b1e391b073.png" width="1080" height="118" class="img_ev3q"></p><p>StreamPark 非常贴心的准备了 Demo SQL 任务，可以直接在刚搭建的平台上运行，从这些细节可以看出社区对用户体验非常重视。前期我们的简单任务都是通过 Flink SQL 来编写及运行，StreamPark 对于 Flink SQL 的支持得非常好，优秀的 SQL 编辑器，创新型的 POM 及 Jar 包依赖管理，可以满足非常多的 SQL 场景下的问题。</p><p>目前我们正在进行元数据层面、权限、UDF等相关的方案调研、设计等</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="02-落地-jar-任务"><strong>02 落地 Jar 任务</strong><a href="#02-落地-jar-任务" class="hash-link" aria-label="Direct link to 02-落地-jar-任务" title="Direct link to 02-落地-jar-任务">​</a></h3><p>由于目前团队的数据开发同学大多有 Java 和 Scala 语言基础，为了更加灵活的开发、更加透明的调优 Flink 任务及覆盖更多场景，我们也快速的落地了基于 Jar 包的构建方式。我们落地分为了两个阶段</p><p>第一阶段：<strong>StreamPark 提供了原生 Apache Flink 项目的支持</strong>，我们将存量的任务Git地址配置至 StreamPark，底层使用 Maven 打包为 Jar 包，创建 StreamPark 的 Apache Flink任务，无缝的进行了迁移。在这个过程中，StreamPark 只是作为了任务提交和状态维护的一个平台工具，远远没有使用到上面提到的其他功能。</p><p>第二阶段：第一阶段将任务都迁移至 StreamPark 上之后，任务已经在平台上运行，但是任务的配置，如 checkpoint，容错以及 Flink 任务内部的业务参数的调整都需要修改源码 push 及 build，效率十分低下且不透明。</p><p>于是，根据 StreamPark 的 QuickStart 我们快速整合了StreamPark 的编程模型，也就是StreamPark Flink 任务（对于 Apache Flink）的封装。</p><p>如：</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">StreamingContext = ParameterTool + StreamExecutionEnvironment</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><ul><li>StreamingContext 为 StreamPark 的封装对象 </li><li>ParameterTool 为解析配置文件后的参数对象 </li></ul><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain"> String value = ParameterTool.get(&quot;${user.custom.key}&quot;) </span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><ul><li>StreamExecutionEnvironment 为 Apache Flink 原生任务上下文</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="03-业务支撑--能力开放"><strong>03 业务支撑 &amp; 能力开放</strong><a href="#03-业务支撑--能力开放" class="hash-link" aria-label="Direct link to 03-业务支撑--能力开放" title="Direct link to 03-业务支撑--能力开放">​</a></h2><p>目前尘锋基于 StreamPark 的实时计算平台从去年11月底上线至今，已经上线 50+ Flink 任务，其中 10+为 Flink SQL 任务，40+ 为 Jar 任务。目前主要还是数据团队内部使用，近期会将实时计算平台开放全公司业务团队使用，任务量会大量增加。</p><p><img loading="lazy" src="/assets/images/online_jar-48549248f657388c7aebc0c8491660fa.png" width="1080" height="445" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="01-实时数仓"><strong>01 实时数仓</strong><a href="#01-实时数仓" class="hash-link" aria-label="Direct link to 01-实时数仓" title="Direct link to 01-实时数仓">​</a></h3><p>时数仓主要是用 Jar 任务，因为模式比较通用，使用 Jar 任务可以通用化的处理大量的数据表同步和计算，甚至做到配置化同步等，我们的实时数仓主要基 Apache Doris 来存储，使用 Flink 来进行清洗计算（目标是存算分离）</p><p>使用 StreamPark 整合其他组件也是非常简单，同时我们也将 Apache Doris 和 Kafka 相关的配置也抽象到了配置文件中，大大提升了我们的开发效率和灵活度。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="02-能力开放"><strong>02 能力开放</strong><a href="#02-能力开放" class="hash-link" aria-label="Direct link to 02-能力开放" title="Direct link to 02-能力开放">​</a></h3><p>数据团队外的其他业务团队也有很多的流处理场景，于是我们将基于 StreamPark 的实时计算平台二次开发后，将以下能力开放全公司业务团队</p><ul><li>业务能力开放：实时数仓上游将所有业务表通过日志采集写入 Kafka，业务团队可基于 Kafka 进行业务相关开发，也可通过实时数仓（Apache Doris）进行 OLAP分析</li><li>计算能力开放：将大数据平台的服务器资源开放业务团队使用</li><li>解决方案开放：Flink 生态的成熟 Connector、Exactly Once 语义支持，可减少业务团队流处理相关的开发成本及维护成本</li></ul><p>目前 StreamPark 还不支持多业务组功能，多业务组功能会抽象后贡献社区。   </p><p><img loading="lazy" src="/assets/images/manager-07ba2a4bc979cd2dd86fc9e07384ec61.png" width="1080" height="235" class="img_ev3q">        </p><p><img loading="lazy" src="/assets/images/task_retrieval-eee86f9c0af117cbf15d2af5d528b2cc.png" width="1080" height="382" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="04-未来规划"><strong>04 未来规划</strong><a href="#04-未来规划" class="hash-link" aria-label="Direct link to 04-未来规划" title="Direct link to 04-未来规划">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="01-flink-on-k8s"><strong>01 Flink on K8S</strong><a href="#01-flink-on-k8s" class="hash-link" aria-label="Direct link to 01-flink-on-k8s" title="Direct link to 01-flink-on-k8s">​</a></h3><p>目前我司 Flink 任务都运行在 Yarn 上，满足当下需求，但 Flink on kubernetes 有以下优点:</p><ul><li><strong>统一运维</strong>。公司统一化运维，有专门的部门运维 K8S</li><li><strong>CPU 隔离</strong>。K8S Pod 之间 CPU 隔离，实时任务不相互影响，更加稳定</li><li><strong>存储计算分离</strong>。Flink 计算资源和状态存储分离，计算资源能够和其他组件资源进行混部，提升机器使用率</li><li><strong>弹性扩缩容</strong>。能够弹性扩缩容，更好的节省人力和物力成本</li></ul><p>目前本人也在整理和落地相关的技术架构及方案，并已在实验环境使用 StreamPark 完成了 Flink on kubernetes 的技术验证，生产落地这一目标由于有 StreamPark 的平台支持，以及社区同学的热心帮心，相信在未来不久就能达成。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="02-流批一体建设"><strong>02 流批一体建设</strong><a href="#02-流批一体建设" class="hash-link" aria-label="Direct link to 02-流批一体建设" title="Direct link to 02-流批一体建设">​</a></h3><p>个人认为批/流最大的区别在于算子 Task 的调度策略 和 数据在算子间的流转策略：</p><ul><li><strong>批处理</strong>上下游算子 Task 存在先后调度（上游Task结束释放资源），数据存在 Shuffle 策略（落地磁盘），缺点是时效性较低且计算无中间状态，但优点是吞吐量大，适合离线超大数据量计算。</li><li><strong>流处理</strong>上下游算子 Task 同时拉起（同时占用资源），数据通过网络在节点间流式计算，缺点是吞吐量不足，优点是时效性高及计算有中间状态，适合实时及增量计算场景。</li></ul><p>如上，个人认为选择<strong>批处理</strong>还是<strong>流处理</strong>，<strong>是数据开发针对不同数据量和不同业务场景的一种调优方式</strong>。但目前由于计算引擎和计算平台会将离线、实时区分，会造成开发及维护的撕裂，成本巨高不下。如果要实现批流一体，要实现以下几个方面：</p><ul><li>存储的统一（元数据的统一）：支持批及流的写入/读取</li><li>计算引擎的统一 ：能够使用一套 API 或 SQL 开发离线和实时任务</li><li>数据平台的统一 ：能够支持实时任务常驻，也能支持离线调度策略</li></ul><p>关于批流统一这一块，目前也正在调研、整理、感兴趣的小伙伴欢迎一块探讨项目学习。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="05-结束语"><strong>05 结束语</strong><a href="#05-结束语" class="hash-link" aria-label="Direct link to 05-结束语" title="Direct link to 05-结束语">​</a></h2><p>以上就是 StreamPark 在尘锋信息生产实践的全部分享内容，感谢大家看到这里。写这篇文章的初心是为大家带来一点 StreamPark 的生产实践的经验和参考，并且和 StreamPark 社区的小伙伴们一道，共同建设 StreamPark ，未来也准备会有更多的参与和建设。非常感谢 StreamPark 的开发者们，能够提供这样优秀的产品，足够多的细节都感受到了大家的用心。虽然目前公司生产使用的（1.2.0-release）版本，在任务分组检索，编辑返回跳页等交互体验上还有些许不足，但瑕不掩瑜，相信 StreamPark 会越来越好，<strong>也相信 StreamPark 会推动 Apache Flink 的普及</strong>。最后用 Apache Flink 社区的一句话来作为结束吧：实时即未来！</p><p><img loading="lazy" src="/assets/images/author-487ad3c1ad9a397cd4c2614f54976368.png" width="844" height="439" class="img_ev3q"></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/stream-park">StreamPark</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/生产实践">生产实践</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/flink-sql">FlinkSQL</a></li></ul></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"><a class="pagination-nav__link pagination-nav__link--next" href="/blog/tags/flink-sql/page/2"><div class="pagination-nav__label">Older Entries</div></a></nav></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title"></div><ul class="footer__items clean-list"><li class="footer__item">
                  <div class="footer-left-box">
                    <div class="flex align-center footer-system">
                      <span class="system-title">About StreamPark</span>
                    </div>
                    <p>Make stream processing easier! easy-to-use streaming application development framework and operation platform</p>
                  </div>
                </li></ul></div><div class="col footer__col"><div class="footer__title">Resource</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Documentation</a></li><li class="footer__item"><a href="https://github.com/apache/incubator-streampark/releases" target="_blank" rel="noopener noreferrer" class="footer__link-item">Releases<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/apache/incubator-streampark/issues/507" target="_blank" rel="noopener noreferrer" class="footer__link-item">FAQ<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/apache/incubator-streampark" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/apache/incubator-streampark/issues" target="_blank" rel="noopener noreferrer" class="footer__link-item">Issue Tracker<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/apache/incubator-streampark/pulls" target="_blank" rel="noopener noreferrer" class="footer__link-item">Pull Requests<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Follow</div><ul class="footer__items clean-list"><li class="footer__item">
                <div class="subscribe-box">
                  <div class="d-flex align-items-center" style="margin-bottom: 30px;padding-top: 11px">
                    <div class="subscribe-input flex-fill">
                      <input class="form-control" id="email_address" maxlength="60" name="email_address" placeholder="Subscribe with us">
                    </div>
                    <div class="subscribe-submit-inner">
                      <a class="btn btn-white m-0" type="submit" href="mailto:dev-subscribe@streampark.apache.org">
                        <span><i class="fa fa-paper-plane text-white"></i></span>
                      </a>
                    </div>
                  </div>
                  <ul class="icon-bottom">
                    <li>
                      <a href="javascript:void(0)">
                        <i class="fa fa-wechat"></i>
                        <div class="wechat-dropdown"><img src="/image/join_wechat.png" alt="weChat"></div>
                      </a>
                    </li>
                    <li><a href="javascript:void(0)"><i class="fa fa-twitter"></i></a></li>
                    <li><a href="javascript:void(0)"><i class="fa fa-slack"></i></a></li>
                    <li><a href="javascript:void(0)"><i class="fa fa-facebook"></i></a></li>
                  </ul>
                </div>
              </li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">
        <div style="text-align: left;margin-top:30px">
          <div class="d-flex align-items-center">
            <div>
              <a href="https://incubator.apache.org/" class="footerLogoLink" one-link-mark="yes">
                <img src="/image/apache-incubator.svg" alt="Apache Incubator logo" class="footer__logo">
              </a>
            </div>
            <div>
              <p style="font-family: Avenir-Medium;font-size: 14px;color: #999;line-height: 25px;">
              Apache StreamPark is an effort undergoing incubation at The Apache Software Foundation (ASF), sponsored by the Apache Incubator. Incubation is required of all newly accepted projects until a further review indicates that the infrastructure, communications, and decision making process have stabilized in a manner consistent with other successful ASF projects. While incubation status is not necessarily a reflection of the completeness or stability of the code, it does indicate that the project has yet to be fully endorsed by the ASF.
              </p>
            </div>
          </div>

          <div style="border-top: 1px solid #525252;min-height: 60px;line-height: 25px;text-align: left;font-family: Avenir-Medium;font-size: 14px;color: #999;display: flex;align-items: center;">
            <span>
              Copyright © 2022-2023 The Apache Software Foundation. Apache StreamPark, StreamPark, and its feather logo are trademarks of The Apache Software Foundation.
            </span>
          </div>
        </div></div></div></div></footer></div>
<script src="/assets/js/runtime~main.f303b739.js"></script>
<script src="/assets/js/main.41b6cbdd.js"></script>
</body>
</html>